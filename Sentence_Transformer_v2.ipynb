{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentence Embeddings using Sentence Transformers ","metadata":{}},{"cell_type":"markdown","source":"### **SBERT - state of the art sentence embedding technique.**","metadata":{}},{"cell_type":"markdown","source":"### Reference_links: <br>\n\nhttps://www.sbert.net/# <br>\nSentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings.\nThe initial work is described in paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\n\n\nProblem with BERT: <br>\n - A common method to address semantic search problems is to map each sentence to a vector space such that semantically similar sentences are close. \n - Researchers have experimented with inputting individual sentences into BERT and to derive fixedsize sentence embeddings. The most commonly used approach is to average the BERT output layer(known as BERT embeddings) or by using the output of the first token (the [CLS] token). \n \n - Above paper shows experimentation of using input of individual sentences into BERT, deriving fixed size sentence embeddings(most popular approach is to average the BERT output layer or by using the output of the first token (the [CLS] token)) is ofthen worse than using average Glove embeddings.\n \nSBERT Overview: <br>\n - Sentence-BERT(SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity.\n \n- Sbert derived sentence embeddings significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018).\n\n- we use the pre-trained BERT and RoBERTa network and only fine-tune it to yield useful sentence embeddings. This reduces significantly the needed training time: SBERT can be tuned in less than 20 minutes, while yielding  better results than comparable sentence embedding methods.\n\n\n\nhttps://www.sbert.net/examples/applications/semantic-search/README.html","metadata":{}},{"cell_type":"markdown","source":"### Command to install sentence- transformers\n","metadata":{}},{"cell_type":"code","source":"pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:41:45.955857Z","iopub.execute_input":"2021-11-25T10:41:45.956574Z","iopub.status.idle":"2021-11-25T10:42:08.075528Z","shell.execute_reply.started":"2021-11-25T10:41:45.956452Z","shell.execute_reply":"2021-11-25T10:42:08.074316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install swifter # for efficient application of pd.apply()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:42:08.077786Z","iopub.execute_input":"2021-11-25T10:42:08.078111Z","iopub.status.idle":"2021-11-25T10:42:19.373674Z","shell.execute_reply.started":"2021-11-25T10:42:08.078065Z","shell.execute_reply":"2021-11-25T10:42:19.37237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nimport torch\nimport pandas as pd\nimport numpy as np\nimport re\nimport bs4\nimport swifter\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:43:52.877299Z","iopub.execute_input":"2021-11-25T10:43:52.877762Z","iopub.status.idle":"2021-11-25T10:44:05.362654Z","shell.execute_reply.started":"2021-11-25T10:43:52.877714Z","shell.execute_reply":"2021-11-25T10:44:05.361534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Data Loading**","metadata":{}},{"cell_type":"code","source":"%%time\n!wget --header=\"Host: 34.125.119.108:5000\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: http://34.125.119.108:5000/edit/Final_df.csv\" \"http://34.125.119.108:5000/files/Final_df.csv?download=1\" -c -O 'Final_df.csv'","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:45:51.599042Z","iopub.execute_input":"2021-11-25T10:45:51.599424Z","iopub.status.idle":"2021-11-25T10:48:56.637619Z","shell.execute_reply.started":"2021-11-25T10:45:51.599382Z","shell.execute_reply":"2021-11-25T10:48:56.635986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf = pd.read_csv('./Final_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:49:05.957881Z","iopub.execute_input":"2021-11-25T10:49:05.958254Z","iopub.status.idle":"2021-11-25T10:49:48.742038Z","shell.execute_reply.started":"2021-11-25T10:49:05.958214Z","shell.execute_reply":"2021-11-25T10:49:48.741134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape, df.columns","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:49:48.743663Z","iopub.execute_input":"2021-11-25T10:49:48.743995Z","iopub.status.idle":"2021-11-25T10:49:48.7525Z","shell.execute_reply.started":"2021-11-25T10:49:48.743961Z","shell.execute_reply":"2021-11-25T10:49:48.750213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Data Preprocessing Functions**\n\n- Removing html tags from question corpus\n- Converting text to lowercase\n- Text decontraction\n- Remove any non-alphanumeric character exept '+', '.' and '#'. These puntuations are kept as we have tags such as c++,c#,.net, vb.net etc. If '+' and '#' is removed all the questions of c# and c++ will be tagged of 'C' programming language which would be a disaster.\n\n- Word lemmatization - all the words will be converted to its stem word.","metadata":{}},{"cell_type":"code","source":"# # https://stackoverflow.com/a/47091490/4084039\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\n\ndef text_preprocessing(text):\n    '''This function does text preprocessing \n       It includes removal of html tags,\n       converting to lowercase, \n       decontraction and \n       removal of any non alphanumeric characters.\n       \n       Function takes one parameter - text\n       returns - preprocessed text\n    '''\n    \n    # Some titles (~42) start with '<' but doesnt have closing '>'. \n    #eg: #text = '<asp: RegularExpressionValidator and RegexOptions.IgnorePatternWhitespace'\n    # beautifulsoup gives emppty string on such text so remove '<' before removing html tags from titles.\n    text = text.replace(\"<\",\"\")\n    # Remove html tags from question corpus\n    text = bs4.BeautifulSoup(text, 'lxml').get_text()\n    # Convert each word to lowercase\n    text = text.lower()\n    # text decontraction. eg: won't to will not. Can't to cannot\n    text = decontracted(text)\n    # Remove any non-alphanumeric characters if present\n    #text = re.sub('\\W', ' ',text).strip()\n    text = re.sub(\"[^a-zA-Z'.+# ]+\", '', text) # kepping + for c++, . for .net, vb.net etc, # for C#\n\n    # why lemmatization is choose over stemming\n    #https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming\n    # Lemmatization   \n    lemmatizer = WordNetLemmatizer()\n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    text = text.strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:49:48.754376Z","iopub.execute_input":"2021-11-25T10:49:48.755023Z","iopub.status.idle":"2021-11-25T10:49:48.769719Z","shell.execute_reply.started":"2021-11-25T10:49:48.75498Z","shell.execute_reply":"2021-11-25T10:49:48.768933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf['Cleaned_Titles'] = df['Title'].swifter.apply(lambda x: text_preprocessing(x))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:49:48.772058Z","iopub.execute_input":"2021-11-25T10:49:48.77287Z","iopub.status.idle":"2021-11-25T10:56:09.23796Z","shell.execute_reply.started":"2021-11-25T10:49:48.77278Z","shell.execute_reply":"2021-11-25T10:56:09.236719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Original: \",df['Title'].iloc[2])\nprint(\"Cleaned: \",df['Cleaned_Titles'].iloc[2])\nprint(\"_____________________________________________________________\")\nprint(\"Original: \",df['Title'].iloc[3])\nprint(\"Cleaned: \",df['Cleaned_Titles'].iloc[3])\n\nprint(\"_____________________________________________________________\")\nprint(\"Original: \",df['Title'].iloc[1000])\nprint(\"Cleaned: \",df['Cleaned_Titles'].iloc[1000])","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:09.240198Z","iopub.execute_input":"2021-11-25T10:56:09.240854Z","iopub.status.idle":"2021-11-25T10:56:09.255205Z","shell.execute_reply.started":"2021-11-25T10:56:09.240809Z","shell.execute_reply":"2021-11-25T10:56:09.251766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncorpus = df['Cleaned_Titles'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:09.25727Z","iopub.execute_input":"2021-11-25T10:56:09.257858Z","iopub.status.idle":"2021-11-25T10:56:09.3154Z","shell.execute_reply.started":"2021-11-25T10:56:09.257814Z","shell.execute_reply":"2021-11-25T10:56:09.314136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(corpus)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:09.317689Z","iopub.execute_input":"2021-11-25T10:56:09.318285Z","iopub.status.idle":"2021-11-25T10:56:09.328678Z","shell.execute_reply.started":"2021-11-25T10:56:09.318225Z","shell.execute_reply":"2021-11-25T10:56:09.327331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndel df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:09.330177Z","iopub.execute_input":"2021-11-25T10:56:09.330484Z","iopub.status.idle":"2021-11-25T10:56:10.797773Z","shell.execute_reply.started":"2021-11-25T10:56:09.330448Z","shell.execute_reply":"2021-11-25T10:56:10.796885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Modelling**\n\n\n\nhttps://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models\n\n- The all-* models where trained on all available training data (more than 1 billion training pairs) and are designed as general purpose models. The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality.\n\n\n#### **Pre-Trained Model Selection**\n\nhttps://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n- all-MiniLM-L6-v2 model is selected because of following advantages and relevancy:- \n    - It was fine-tuned on datasets relevant to our problem statement such as stackexchange, reddit comments, yahoo answers etc.\n    - It is faster\n    - Intended to use for semantic search\n    \n    \n#### **Model Background**\n- all-MiniLM-L6-v2 model is fine tuned on pretrained nreimers/MiniLM-L6-H384-uncased model on a 1B sentence pairs dataset. Contrastive learning objective has been used: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\n- Formally,cosine similarity from each possible sentence pairs from the batch was computed. Then cross entropy loss was applied by comparing with true pairs. ","metadata":{}},{"cell_type":"markdown","source":"## **Model Loading**","metadata":{}},{"cell_type":"code","source":"%%time\nembedder = SentenceTransformer('all-MiniLM-L6-v2')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:10.799197Z","iopub.execute_input":"2021-11-25T10:56:10.799738Z","iopub.status.idle":"2021-11-25T10:56:42.507054Z","shell.execute_reply.started":"2021-11-25T10:56:10.799698Z","shell.execute_reply":"2021-11-25T10:56:42.505719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Extracting Sentence Embedding**","metadata":{}},{"cell_type":"code","source":"# %%time\n# corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:42.510602Z","iopub.execute_input":"2021-11-25T10:56:42.510974Z","iopub.status.idle":"2021-11-25T10:56:42.517387Z","shell.execute_reply.started":"2021-11-25T10:56:42.510931Z","shell.execute_reply":"2021-11-25T10:56:42.515998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving sentence embeddings for future use","metadata":{}},{"cell_type":"code","source":"# import pickle\n# with open(\"sentence-embeddings.pkl\", \"wb\") as fOut:\n#     pickle.dump({'sentences': corpus, 'embeddings': corpus_embeddings},fOut)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:42.51902Z","iopub.execute_input":"2021-11-25T10:56:42.519317Z","iopub.status.idle":"2021-11-25T10:56:42.533115Z","shell.execute_reply.started":"2021-11-25T10:56:42.519283Z","shell.execute_reply":"2021-11-25T10:56:42.532283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(\"../input/sbert-embeddings/sentence-embeddings.pkl\", \"rb\") as f:\n    corpus_dict = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:42.534316Z","iopub.execute_input":"2021-11-25T10:56:42.534618Z","iopub.status.idle":"2021-11-25T10:56:55.958747Z","shell.execute_reply.started":"2021-11-25T10:56:42.534577Z","shell.execute_reply":"2021-11-25T10:56:55.957452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_embeddings = corpus_dict['embeddings']","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:55.960446Z","iopub.execute_input":"2021-11-25T10:56:55.960779Z","iopub.status.idle":"2021-11-25T10:56:56.05711Z","shell.execute_reply.started":"2021-11-25T10:56:55.960733Z","shell.execute_reply":"2021-11-25T10:56:56.05597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(corpus_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:56.05913Z","iopub.execute_input":"2021-11-25T10:56:56.059549Z","iopub.status.idle":"2021-11-25T10:56:56.356527Z","shell.execute_reply.started":"2021-11-25T10:56:56.059493Z","shell.execute_reply":"2021-11-25T10:56:56.355649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(corpus_embeddings[0]) # sentence embedding of 384 dimensional","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:56.35806Z","iopub.execute_input":"2021-11-25T10:56:56.358807Z","iopub.status.idle":"2021-11-25T10:56:56.371185Z","shell.execute_reply.started":"2021-11-25T10:56:56.358763Z","shell.execute_reply":"2021-11-25T10:56:56.37045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:-\n- Model 'all-MiniLM-L6-v2' gives 384 dimensional sentence embeddings.\n- Each question title has been converted into 384 dim embeddings.","metadata":{}},{"cell_type":"markdown","source":"### **Model Performance**","metadata":{}},{"cell_type":"code","source":"def get_similar_questions(query):\n    ''' Function to accept user query and show top 5 similar question alongwith cosine similarity score.\n        Function accepts one parameter: query (text input)\n        Processing: Text preprocessing of query, compute sentence embedding and top similar 5 questions.\n        Returns: None, prints similar question's titles and cosine similarity score.\n    '''\n    preprocessed_query = text_preprocessing(query)\n    query_embedding = embedder.encode(query, convert_to_tensor=True)\n    # We use cosine-similarity and torch.topk to find the highest 5 scores\n    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n    \n    \n    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n    top_k = 5\n    top_results = torch.topk(cos_scores, k=top_k)\n    \n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar questions in corpus:\")\n    \n    i = 1\n    for score, idx in zip(top_results[0], top_results[1]):\n        print(\"{}) \".format(i), corpus[idx], \"(Score: {:.4f})\".format(score))\n        i = i+1","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:56.375425Z","iopub.execute_input":"2021-11-25T10:56:56.376015Z","iopub.status.idle":"2021-11-25T10:56:56.428984Z","shell.execute_reply.started":"2021-11-25T10:56:56.375972Z","shell.execute_reply":"2021-11-25T10:56:56.427834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('python sort dictionary')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:56.430513Z","iopub.execute_input":"2021-11-25T10:56:56.430898Z","iopub.status.idle":"2021-11-25T10:56:58.687538Z","shell.execute_reply.started":"2021-11-25T10:56:56.430742Z","shell.execute_reply":"2021-11-25T10:56:58.686431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('CSS Performance')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:56:58.688979Z","iopub.execute_input":"2021-11-25T10:56:58.689224Z","iopub.status.idle":"2021-11-25T10:57:00.105131Z","shell.execute_reply.started":"2021-11-25T10:56:58.689193Z","shell.execute_reply":"2021-11-25T10:57:00.104172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('python convert date to datetime')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:57:00.107151Z","iopub.execute_input":"2021-11-25T10:57:00.108378Z","iopub.status.idle":"2021-11-25T10:57:01.503159Z","shell.execute_reply.started":"2021-11-25T10:57:00.108277Z","shell.execute_reply":"2021-11-25T10:57:01.502042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('how to create list of lists in python')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:57:01.505697Z","iopub.execute_input":"2021-11-25T10:57:01.506083Z","iopub.status.idle":"2021-11-25T10:57:02.956996Z","shell.execute_reply.started":"2021-11-25T10:57:01.506038Z","shell.execute_reply":"2021-11-25T10:57:02.955279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"%%time\nget_similar_questions('use groupingby with custom logic in java')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:57:02.959143Z","iopub.execute_input":"2021-11-25T10:57:02.959674Z","iopub.status.idle":"2021-11-25T10:57:04.505781Z","shell.execute_reply.started":"2021-11-25T10:57:02.959626Z","shell.execute_reply":"2021-11-25T10:57:04.504663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('str(a) giving unicode error')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:57:04.507213Z","iopub.execute_input":"2021-11-25T10:57:04.507912Z","iopub.status.idle":"2021-11-25T10:57:05.932193Z","shell.execute_reply.started":"2021-11-25T10:57:04.50787Z","shell.execute_reply":"2021-11-25T10:57:05.931105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('pd.melt() not working python')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:57:05.933877Z","iopub.execute_input":"2021-11-25T10:57:05.934134Z","iopub.status.idle":"2021-11-25T10:57:07.352077Z","shell.execute_reply.started":"2021-11-25T10:57:05.934104Z","shell.execute_reply":"2021-11-25T10:57:07.351029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('try: 22/0 except Exception as e:print(\"Error! Code: {c}, Message, {m}\".format(c = e.code, m = str(e))')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:57:07.353809Z","iopub.execute_input":"2021-11-25T10:57:07.354076Z","iopub.status.idle":"2021-11-25T10:57:08.664542Z","shell.execute_reply.started":"2021-11-25T10:57:07.354043Z","shell.execute_reply":"2021-11-25T10:57:08.663257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('def main(): return {a:1, b:2}')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:57:27.241578Z","iopub.execute_input":"2021-11-25T10:57:27.241902Z","iopub.status.idle":"2021-11-25T10:57:28.701218Z","shell.execute_reply.started":"2021-11-25T10:57:27.241871Z","shell.execute_reply":"2021-11-25T10:57:28.700117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nget_similar_questions('import KNN \\\n                       knn= KNN(n=4) \\\n                       knn.fit(Xtrain, ytrain)')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T10:57:09.90029Z","iopub.execute_input":"2021-11-25T10:57:09.900614Z","iopub.status.idle":"2021-11-25T10:57:11.316679Z","shell.execute_reply.started":"2021-11-25T10:57:09.900536Z","shell.execute_reply":"2021-11-25T10:57:11.315634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Observations:-**\n- For query - 'python sort dictionary'\n    - Top 5 similar questions retrieved have similarity score > 90. Model is confident about the results given as output.\n    - All the questions are pretty much similar to query provided.\n\n- For query - 'python convert date to datetime'\n    - Key thing to notice is, result set didnt included completely opposite question such as conversion of datetime to date while other techniques like avgw2vec faced this issue.\n    \n    \n- For query - 'how to create list of lists in python'\n    - Top most similar question list of list in pythonic way - indirect connection to list comprehension.\n    - Though avgw2v method had captured this connection.\n    \n - Code related query - 'str(a) giving unicode error'\n     - Resutls are impressive.\n     - All top 5 questions was related to unicode error.\n\n\n-  Code related query: 'pd.melt() not working python'\n    - Query was very specific to python pandas, corpus may not have data related to it.\n    - Still the top similar question retrived is very much similar to query inputted.","metadata":{}},{"cell_type":"markdown","source":"# **Final Conclusion**\n\n\n- Best part of using this technique is the time it takes to retrieve results. Result are retrieved within 2-3 seconds which is pretty amazing satisfying our bussiness constraint of low latency. \n\n    - Main character of this super fast result retriveal is torch.topK function. It is very much efficient and optimized to return largest K values from the provided tensor.\n\n    - Whereas W2vec technique was taking around 4.5 mins to retrieve top 5 similar questions.\n    \n- Model size is around 80Mb only, compatible with deployment.\n\n- Model gave fairly good results.\n\n","metadata":{}}]}