{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Trained Word2Vec Model\n",
    "\n",
    "- without nonalphanumeric characters removal in preprocessing step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-11-09T13:19:33.553825Z",
     "iopub.status.busy": "2021-11-09T13:19:33.553537Z",
     "iopub.status.idle": "2021-11-09T13:19:35.999682Z",
     "shell.execute_reply": "2021-11-09T13:19:35.998821Z",
     "shell.execute_reply.started": "2021-11-09T13:19:33.553791Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import bs4\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-11-09T13:17:16.735761Z",
     "iopub.status.busy": "2021-11-09T13:17:16.734814Z",
     "iopub.status.idle": "2021-11-09T13:17:50.346023Z",
     "shell.execute_reply": "2021-11-09T13:17:50.345193Z",
     "shell.execute_reply.started": "2021-11-09T13:17:16.735719Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T13:17:50.348136Z",
     "iopub.status.busy": "2021-11-09T13:17:50.347845Z",
     "iopub.status.idle": "2021-11-09T13:17:50.357308Z",
     "shell.execute_reply": "2021-11-09T13:17:50.356466Z",
     "shell.execute_reply.started": "2021-11-09T13:17:50.348099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 9),\n",
       " Index(['PostTypeId', 'Question_Id', 'Title', 'Tags', 'AnswerCount',\n",
       "        'ViewCount', 'Body', 'Score', 'Answer_corpus'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training - On Question Corpus\n",
    "### Ideal - Question corpus + answer corpus but memory constraints didnt allow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T13:17:50.359227Z",
     "iopub.status.busy": "2021-11-09T13:17:50.358673Z",
     "iopub.status.idle": "2021-11-09T13:17:51.376424Z",
     "shell.execute_reply": "2021-11-09T13:17:51.375631Z",
     "shell.execute_reply.started": "2021-11-09T13:17:50.359188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 815 ms, sys: 772 ms, total: 1.59 s\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['Question_Corpus'] = df['Title'] + \" \" + df['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T13:17:51.378612Z",
     "iopub.status.busy": "2021-11-09T13:17:51.378330Z",
     "iopub.status.idle": "2021-11-09T13:17:51.384573Z",
     "shell.execute_reply": "2021-11-09T13:17:51.383730Z",
     "shell.execute_reply.started": "2021-11-09T13:17:51.378571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How do I calculate someone's age based on a DateTime type birthday? <p>Given a <code>DateTime</code> representing a person's birthday, how do I calculate their age in years?</p>\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Question_Corpus'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Removing html tags from question corpus\n",
    "- Converting text to lowercase\n",
    "- Text decontraction\n",
    "- Not removing any punctuations as '+' character is useful as we have questions tagged as c++, c#, .net etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-11-09T13:19:55.082822Z",
     "iopub.status.busy": "2021-11-09T13:19:55.082453Z",
     "iopub.status.idle": "2021-11-09T13:19:55.097827Z",
     "shell.execute_reply": "2021-11-09T13:19:55.096962Z",
     "shell.execute_reply.started": "2021-11-09T13:19:55.082783Z"
    }
   },
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/a/47091490/4084039\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    '''This function does text preprocessing \n",
    "       It includes removal of html tags,\n",
    "       converting to lowercase, \n",
    "       decontraction and \n",
    "       removal of any non alphanumeric characters.\n",
    "       \n",
    "       Function takes one parameter - text\n",
    "       returns - preprocessed text\n",
    "    '''\n",
    "    # Remove html tags from question corpus\n",
    "    text = bs4.BeautifulSoup(text, 'lxml').get_text()\n",
    "    # Convert each word to lowercase\n",
    "    text = text.lower()\n",
    "    # text decontraction. eg: won't to will not. Can't to cannot\n",
    "    text = decontracted(text)\n",
    "    # Remove any non-alphanumeric characters if present\n",
    "    #text = re.sub('\\W', ' ',text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T13:20:00.199429Z",
     "iopub.status.busy": "2021-11-09T13:20:00.198636Z",
     "iopub.status.idle": "2021-11-09T13:20:00.205053Z",
     "shell.execute_reply": "2021-11-09T13:20:00.204244Z",
     "shell.execute_reply.started": "2021-11-09T13:20:00.199387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PostTypeId', 'Question_Id', 'Title', 'Tags', 'AnswerCount',\n",
       "       'ViewCount', 'Body', 'Score', 'Answer_corpus', 'Question_Corpus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T13:20:01.522766Z",
     "iopub.status.busy": "2021-11-09T13:20:01.522286Z",
     "iopub.status.idle": "2021-11-09T13:30:02.053974Z",
     "shell.execute_reply": "2021-11-09T13:30:02.052868Z",
     "shell.execute_reply.started": "2021-11-09T13:20:01.522730Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [09:14<00:00, 1802.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 11s, sys: 5.95 s, total: 9min 17s\n",
      "Wall time: 9min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['Cleaned_corpus'] = df['Question_Corpus'].progress_apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    how do i calculate someone's age based on a da...\n",
       "1    calculate relative time in c# given a specific...\n",
       "2    determine a user's timezone is there a standar...\n",
       "3    difference between math.floor() and math.trunc...\n",
       "4    filling a dataset or a datatable from a linq q...\n",
       "Name: Cleaned_corpus, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cleaned_corpus'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2vec accepts input in list of list format where inner lists should contain word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 3.91 s, total: 24.7 s\n",
      "Wall time: 24.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "corpus = df['Cleaned_corpus'].str.split().tolist()\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To discover system cores for parallel processing\n",
    "#https://stackoverflow.com/questions/53417258/what-is-workers-parameter-in-word2vec-in-nlp\n",
    "from gensim.utils import effective_n_jobs\n",
    "effective_n_jobs(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T13:39:50.783122Z",
     "iopub.status.busy": "2021-11-09T13:39:50.782694Z",
     "iopub.status.idle": "2021-11-09T13:44:56.625248Z",
     "shell.execute_reply": "2021-11-09T13:44:56.624568Z",
     "shell.execute_reply.started": "2021-11-09T13:39:50.783084Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08:49:49: collecting all words and their counts\n",
      "INFO - 08:49:49: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 08:49:49: PROGRESS: at sentence #10000, processed 1124216 words, keeping 88758 word types\n",
      "INFO - 08:49:49: PROGRESS: at sentence #20000, processed 2246839 words, keeping 153620 word types\n",
      "INFO - 08:49:50: PROGRESS: at sentence #30000, processed 3421690 words, keeping 217748 word types\n",
      "INFO - 08:49:50: PROGRESS: at sentence #40000, processed 4614264 words, keeping 280935 word types\n",
      "INFO - 08:49:50: PROGRESS: at sentence #50000, processed 5828887 words, keeping 345263 word types\n",
      "INFO - 08:49:51: PROGRESS: at sentence #60000, processed 7103150 words, keeping 410973 word types\n",
      "INFO - 08:49:51: PROGRESS: at sentence #70000, processed 8377956 words, keeping 476008 word types\n",
      "INFO - 08:49:51: PROGRESS: at sentence #80000, processed 9679157 words, keeping 540692 word types\n",
      "INFO - 08:49:52: PROGRESS: at sentence #90000, processed 10993264 words, keeping 608714 word types\n",
      "INFO - 08:49:52: PROGRESS: at sentence #100000, processed 12263701 words, keeping 671023 word types\n",
      "INFO - 08:49:52: PROGRESS: at sentence #110000, processed 13554201 words, keeping 738370 word types\n",
      "INFO - 08:49:53: PROGRESS: at sentence #120000, processed 14842733 words, keeping 801926 word types\n",
      "INFO - 08:49:53: PROGRESS: at sentence #130000, processed 16137021 words, keeping 867447 word types\n",
      "INFO - 08:49:53: PROGRESS: at sentence #140000, processed 17402870 words, keeping 930601 word types\n",
      "INFO - 08:49:54: PROGRESS: at sentence #150000, processed 18667716 words, keeping 994430 word types\n",
      "INFO - 08:49:54: PROGRESS: at sentence #160000, processed 19908597 words, keeping 1055889 word types\n",
      "INFO - 08:49:54: PROGRESS: at sentence #170000, processed 21180780 words, keeping 1120875 word types\n",
      "INFO - 08:49:55: PROGRESS: at sentence #180000, processed 22484252 words, keeping 1186472 word types\n",
      "INFO - 08:49:55: PROGRESS: at sentence #190000, processed 23767158 words, keeping 1251556 word types\n",
      "INFO - 08:49:56: PROGRESS: at sentence #200000, processed 25079176 words, keeping 1318649 word types\n",
      "INFO - 08:49:56: PROGRESS: at sentence #210000, processed 26364181 words, keeping 1382842 word types\n",
      "INFO - 08:49:56: PROGRESS: at sentence #220000, processed 27644541 words, keeping 1446099 word types\n",
      "INFO - 08:49:57: PROGRESS: at sentence #230000, processed 28930984 words, keeping 1508591 word types\n",
      "INFO - 08:49:57: PROGRESS: at sentence #240000, processed 30209844 words, keeping 1572265 word types\n",
      "INFO - 08:49:57: PROGRESS: at sentence #250000, processed 31468607 words, keeping 1633736 word types\n",
      "INFO - 08:49:58: PROGRESS: at sentence #260000, processed 32751175 words, keeping 1698722 word types\n",
      "INFO - 08:49:58: PROGRESS: at sentence #270000, processed 33995719 words, keeping 1760164 word types\n",
      "INFO - 08:49:58: PROGRESS: at sentence #280000, processed 35283597 words, keeping 1823961 word types\n",
      "INFO - 08:49:59: PROGRESS: at sentence #290000, processed 36555807 words, keeping 1887019 word types\n",
      "INFO - 08:49:59: PROGRESS: at sentence #300000, processed 37821232 words, keeping 1949940 word types\n",
      "INFO - 08:50:00: PROGRESS: at sentence #310000, processed 39113780 words, keeping 2013812 word types\n",
      "INFO - 08:50:00: PROGRESS: at sentence #320000, processed 40389808 words, keeping 2077647 word types\n",
      "INFO - 08:50:00: PROGRESS: at sentence #330000, processed 41642224 words, keeping 2139194 word types\n",
      "INFO - 08:50:01: PROGRESS: at sentence #340000, processed 42911676 words, keeping 2201411 word types\n",
      "INFO - 08:50:01: PROGRESS: at sentence #350000, processed 44225219 words, keeping 2268471 word types\n",
      "INFO - 08:50:01: PROGRESS: at sentence #360000, processed 45473500 words, keeping 2329316 word types\n",
      "INFO - 08:50:02: PROGRESS: at sentence #370000, processed 46758531 words, keeping 2392898 word types\n",
      "INFO - 08:50:02: PROGRESS: at sentence #380000, processed 48047562 words, keeping 2456766 word types\n",
      "INFO - 08:50:02: PROGRESS: at sentence #390000, processed 49316514 words, keeping 2519979 word types\n",
      "INFO - 08:50:03: PROGRESS: at sentence #400000, processed 50593281 words, keeping 2583868 word types\n",
      "INFO - 08:50:03: PROGRESS: at sentence #410000, processed 51911890 words, keeping 2647316 word types\n",
      "INFO - 08:50:04: PROGRESS: at sentence #420000, processed 53209079 words, keeping 2710295 word types\n",
      "INFO - 08:50:04: PROGRESS: at sentence #430000, processed 54493167 words, keeping 2772343 word types\n",
      "INFO - 08:50:04: PROGRESS: at sentence #440000, processed 55785672 words, keeping 2834843 word types\n",
      "INFO - 08:50:05: PROGRESS: at sentence #450000, processed 57066261 words, keeping 2893709 word types\n",
      "INFO - 08:50:05: PROGRESS: at sentence #460000, processed 58375484 words, keeping 2957770 word types\n",
      "INFO - 08:50:06: PROGRESS: at sentence #470000, processed 59660194 words, keeping 3019102 word types\n",
      "INFO - 08:50:06: PROGRESS: at sentence #480000, processed 60958367 words, keeping 3083567 word types\n",
      "INFO - 08:50:06: PROGRESS: at sentence #490000, processed 62253306 words, keeping 3146213 word types\n",
      "INFO - 08:50:07: PROGRESS: at sentence #500000, processed 63531523 words, keeping 3206585 word types\n",
      "INFO - 08:50:07: PROGRESS: at sentence #510000, processed 64778370 words, keeping 3266382 word types\n",
      "INFO - 08:50:07: PROGRESS: at sentence #520000, processed 66041824 words, keeping 3327410 word types\n",
      "INFO - 08:50:08: PROGRESS: at sentence #530000, processed 67314545 words, keeping 3389565 word types\n",
      "INFO - 08:50:08: PROGRESS: at sentence #540000, processed 68618193 words, keeping 3454835 word types\n",
      "INFO - 08:50:08: PROGRESS: at sentence #550000, processed 69918263 words, keeping 3520414 word types\n",
      "INFO - 08:50:09: PROGRESS: at sentence #560000, processed 71192637 words, keeping 3582551 word types\n",
      "INFO - 08:50:09: PROGRESS: at sentence #570000, processed 72450871 words, keeping 3642473 word types\n",
      "INFO - 08:50:10: PROGRESS: at sentence #580000, processed 73738063 words, keeping 3704967 word types\n",
      "INFO - 08:50:10: PROGRESS: at sentence #590000, processed 75030340 words, keeping 3767308 word types\n",
      "INFO - 08:50:10: PROGRESS: at sentence #600000, processed 76319807 words, keeping 3827766 word types\n",
      "INFO - 08:50:11: PROGRESS: at sentence #610000, processed 77600428 words, keeping 3889113 word types\n",
      "INFO - 08:50:11: PROGRESS: at sentence #620000, processed 78907321 words, keeping 3951308 word types\n",
      "INFO - 08:50:11: PROGRESS: at sentence #630000, processed 80186431 words, keeping 4011806 word types\n",
      "INFO - 08:50:12: PROGRESS: at sentence #640000, processed 81450152 words, keeping 4072564 word types\n",
      "INFO - 08:50:12: PROGRESS: at sentence #650000, processed 82708826 words, keeping 4132302 word types\n",
      "INFO - 08:50:13: PROGRESS: at sentence #660000, processed 83997886 words, keeping 4194731 word types\n",
      "INFO - 08:50:13: PROGRESS: at sentence #670000, processed 85270550 words, keeping 4255890 word types\n",
      "INFO - 08:50:13: PROGRESS: at sentence #680000, processed 86542235 words, keeping 4317942 word types\n",
      "INFO - 08:50:14: PROGRESS: at sentence #690000, processed 87836386 words, keeping 4379000 word types\n",
      "INFO - 08:50:14: PROGRESS: at sentence #700000, processed 89103365 words, keeping 4438132 word types\n",
      "INFO - 08:50:14: PROGRESS: at sentence #710000, processed 90392203 words, keeping 4499184 word types\n",
      "INFO - 08:50:15: PROGRESS: at sentence #720000, processed 91675038 words, keeping 4558824 word types\n",
      "INFO - 08:50:15: PROGRESS: at sentence #730000, processed 92961500 words, keeping 4620731 word types\n",
      "INFO - 08:50:15: PROGRESS: at sentence #740000, processed 94256786 words, keeping 4680382 word types\n",
      "INFO - 08:50:16: PROGRESS: at sentence #750000, processed 95528906 words, keeping 4740759 word types\n",
      "INFO - 08:50:16: PROGRESS: at sentence #760000, processed 96816033 words, keeping 4801735 word types\n",
      "INFO - 08:50:17: PROGRESS: at sentence #770000, processed 98094109 words, keeping 4861460 word types\n",
      "INFO - 08:50:17: PROGRESS: at sentence #780000, processed 99374563 words, keeping 4921884 word types\n",
      "INFO - 08:50:17: PROGRESS: at sentence #790000, processed 100685201 words, keeping 4986736 word types\n",
      "INFO - 08:50:18: PROGRESS: at sentence #800000, processed 101953628 words, keeping 5045583 word types\n",
      "INFO - 08:50:18: PROGRESS: at sentence #810000, processed 103250013 words, keeping 5109623 word types\n",
      "INFO - 08:50:18: PROGRESS: at sentence #820000, processed 104534055 words, keeping 5169435 word types\n",
      "INFO - 08:50:19: PROGRESS: at sentence #830000, processed 105825697 words, keeping 5230012 word types\n",
      "INFO - 08:50:19: PROGRESS: at sentence #840000, processed 107126617 words, keeping 5291043 word types\n",
      "INFO - 08:50:20: PROGRESS: at sentence #850000, processed 108396999 words, keeping 5349061 word types\n",
      "INFO - 08:50:20: PROGRESS: at sentence #860000, processed 109676657 words, keeping 5408314 word types\n",
      "INFO - 08:50:20: PROGRESS: at sentence #870000, processed 110981926 words, keeping 5470799 word types\n",
      "INFO - 08:50:21: PROGRESS: at sentence #880000, processed 112295561 words, keeping 5532812 word types\n",
      "INFO - 08:50:21: PROGRESS: at sentence #890000, processed 113606684 words, keeping 5593924 word types\n",
      "INFO - 08:50:22: PROGRESS: at sentence #900000, processed 114906822 words, keeping 5655714 word types\n",
      "INFO - 08:50:22: PROGRESS: at sentence #910000, processed 116208068 words, keeping 5715364 word types\n",
      "INFO - 08:50:23: PROGRESS: at sentence #920000, processed 117516789 words, keeping 5776040 word types\n",
      "INFO - 08:50:23: PROGRESS: at sentence #930000, processed 118837084 words, keeping 5838085 word types\n",
      "INFO - 08:50:23: PROGRESS: at sentence #940000, processed 120164494 words, keeping 5899690 word types\n",
      "INFO - 08:50:24: PROGRESS: at sentence #950000, processed 121485194 words, keeping 5962934 word types\n",
      "INFO - 08:50:24: PROGRESS: at sentence #960000, processed 122800701 words, keeping 6024843 word types\n",
      "INFO - 08:50:24: PROGRESS: at sentence #970000, processed 124106464 words, keeping 6084651 word types\n",
      "INFO - 08:50:25: PROGRESS: at sentence #980000, processed 125435943 words, keeping 6148179 word types\n",
      "INFO - 08:50:25: PROGRESS: at sentence #990000, processed 126737780 words, keeping 6209456 word types\n",
      "INFO - 08:50:26: collected 6273648 word types from a corpus of 128082650 raw words and 1000000 sentences\n",
      "INFO - 08:50:26: Loading a fresh vocabulary\n",
      "INFO - 08:50:46: effective_min_count=2 retains 1747920 unique words (27% of original 6273648, drops 4525728)\n",
      "INFO - 08:50:46: effective_min_count=2 leaves 123556922 word corpus (96% of original 128082650, drops 4525728)\n",
      "INFO - 08:50:52: deleting the raw counts dictionary of 6273648 items\n",
      "INFO - 08:50:53: sample=0.001 downsamples 44 most-common words\n",
      "INFO - 08:50:53: downsampling leaves estimated 96930979 word corpus (78.5% of prior 123556922)\n",
      "INFO - 08:51:01: estimated required memory for 1747920 words and 300 dimensions: 5068968000 bytes\n",
      "INFO - 08:51:01: resetting layer weights\n",
      "INFO - 08:58:16: training model with 8 workers on 1747920 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 08:58:17: EPOCH 1 - PROGRESS: at 0.81% examples, 676304 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:18: EPOCH 1 - PROGRESS: at 1.97% examples, 825093 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:19: EPOCH 1 - PROGRESS: at 3.04% examples, 862641 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:20: EPOCH 1 - PROGRESS: at 4.11% examples, 885955 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:21: EPOCH 1 - PROGRESS: at 5.11% examples, 892176 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:22: EPOCH 1 - PROGRESS: at 6.09% examples, 900792 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 08:58:23: EPOCH 1 - PROGRESS: at 7.03% examples, 901209 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:24: EPOCH 1 - PROGRESS: at 8.01% examples, 906552 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:25: EPOCH 1 - PROGRESS: at 8.99% examples, 912758 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:26: EPOCH 1 - PROGRESS: at 9.99% examples, 915944 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:27: EPOCH 1 - PROGRESS: at 10.93% examples, 916015 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 08:58:28: EPOCH 1 - PROGRESS: at 11.88% examples, 916725 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:29: EPOCH 1 - PROGRESS: at 12.88% examples, 918243 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 08:58:30: EPOCH 1 - PROGRESS: at 13.92% examples, 923464 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:31: EPOCH 1 - PROGRESS: at 14.93% examples, 924364 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:32: EPOCH 1 - PROGRESS: at 15.92% examples, 924430 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:33: EPOCH 1 - PROGRESS: at 16.92% examples, 925869 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:34: EPOCH 1 - PROGRESS: at 17.85% examples, 925415 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:35: EPOCH 1 - PROGRESS: at 18.83% examples, 925080 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:36: EPOCH 1 - PROGRESS: at 19.78% examples, 925392 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:37: EPOCH 1 - PROGRESS: at 20.75% examples, 926125 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:38: EPOCH 1 - PROGRESS: at 21.73% examples, 927465 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:39: EPOCH 1 - PROGRESS: at 22.67% examples, 926905 words/s, in_qsize 15, out_qsize 1\n",
      "INFO - 08:58:40: EPOCH 1 - PROGRESS: at 23.61% examples, 925848 words/s, in_qsize 9, out_qsize 6\n",
      "INFO - 08:58:41: EPOCH 1 - PROGRESS: at 24.62% examples, 927382 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:42: EPOCH 1 - PROGRESS: at 25.58% examples, 927327 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:43: EPOCH 1 - PROGRESS: at 26.54% examples, 926798 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:44: EPOCH 1 - PROGRESS: at 27.54% examples, 927368 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:45: EPOCH 1 - PROGRESS: at 28.51% examples, 927308 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:46: EPOCH 1 - PROGRESS: at 29.49% examples, 927139 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:47: EPOCH 1 - PROGRESS: at 30.44% examples, 927211 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:48: EPOCH 1 - PROGRESS: at 31.41% examples, 927143 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:49: EPOCH 1 - PROGRESS: at 32.40% examples, 927671 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:50: EPOCH 1 - PROGRESS: at 33.35% examples, 926658 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:51: EPOCH 1 - PROGRESS: at 34.30% examples, 926308 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:52: EPOCH 1 - PROGRESS: at 35.26% examples, 925958 words/s, in_qsize 16, out_qsize 3\n",
      "INFO - 08:58:53: EPOCH 1 - PROGRESS: at 36.27% examples, 926653 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:54: EPOCH 1 - PROGRESS: at 37.23% examples, 926538 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:55: EPOCH 1 - PROGRESS: at 38.18% examples, 926372 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 08:58:56: EPOCH 1 - PROGRESS: at 39.17% examples, 926577 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:57: EPOCH 1 - PROGRESS: at 40.08% examples, 925352 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:58:58: EPOCH 1 - PROGRESS: at 41.03% examples, 925759 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:58:59: EPOCH 1 - PROGRESS: at 41.94% examples, 924704 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 08:59:00: EPOCH 1 - PROGRESS: at 42.89% examples, 924556 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 08:59:01: EPOCH 1 - PROGRESS: at 43.83% examples, 924128 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:02: EPOCH 1 - PROGRESS: at 44.81% examples, 924438 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:03: EPOCH 1 - PROGRESS: at 45.76% examples, 924744 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:04: EPOCH 1 - PROGRESS: at 46.72% examples, 924739 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:05: EPOCH 1 - PROGRESS: at 47.67% examples, 924955 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:06: EPOCH 1 - PROGRESS: at 48.65% examples, 925700 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:07: EPOCH 1 - PROGRESS: at 49.59% examples, 925061 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:08: EPOCH 1 - PROGRESS: at 50.57% examples, 925073 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:09: EPOCH 1 - PROGRESS: at 51.57% examples, 925183 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 08:59:10: EPOCH 1 - PROGRESS: at 52.55% examples, 925490 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 08:59:11: EPOCH 1 - PROGRESS: at 53.51% examples, 925581 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:12: EPOCH 1 - PROGRESS: at 54.46% examples, 925620 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:13: EPOCH 1 - PROGRESS: at 55.40% examples, 925416 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:14: EPOCH 1 - PROGRESS: at 56.39% examples, 925670 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 08:59:15: EPOCH 1 - PROGRESS: at 57.38% examples, 926045 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:16: EPOCH 1 - PROGRESS: at 58.31% examples, 925591 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:17: EPOCH 1 - PROGRESS: at 59.25% examples, 925158 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:18: EPOCH 1 - PROGRESS: at 60.23% examples, 925545 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:19: EPOCH 1 - PROGRESS: at 61.19% examples, 925710 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:20: EPOCH 1 - PROGRESS: at 62.13% examples, 925546 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 08:59:21: EPOCH 1 - PROGRESS: at 63.11% examples, 925838 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:22: EPOCH 1 - PROGRESS: at 64.09% examples, 925913 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:23: EPOCH 1 - PROGRESS: at 65.08% examples, 925953 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 08:59:24: EPOCH 1 - PROGRESS: at 66.03% examples, 926112 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:25: EPOCH 1 - PROGRESS: at 67.02% examples, 926380 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:26: EPOCH 1 - PROGRESS: at 67.99% examples, 926165 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:27: EPOCH 1 - PROGRESS: at 68.92% examples, 925943 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:28: EPOCH 1 - PROGRESS: at 69.89% examples, 925787 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:29: EPOCH 1 - PROGRESS: at 70.84% examples, 925604 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:30: EPOCH 1 - PROGRESS: at 71.80% examples, 925620 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:31: EPOCH 1 - PROGRESS: at 72.71% examples, 925156 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:32: EPOCH 1 - PROGRESS: at 73.67% examples, 925277 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:33: EPOCH 1 - PROGRESS: at 74.64% examples, 925462 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:34: EPOCH 1 - PROGRESS: at 75.61% examples, 925653 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:35: EPOCH 1 - PROGRESS: at 76.56% examples, 925413 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:36: EPOCH 1 - PROGRESS: at 77.52% examples, 925461 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:37: EPOCH 1 - PROGRESS: at 78.49% examples, 925637 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:38: EPOCH 1 - PROGRESS: at 79.44% examples, 925559 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:39: EPOCH 1 - PROGRESS: at 80.39% examples, 925304 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:40: EPOCH 1 - PROGRESS: at 81.34% examples, 925126 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:41: EPOCH 1 - PROGRESS: at 82.31% examples, 925143 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:42: EPOCH 1 - PROGRESS: at 83.29% examples, 925532 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:43: EPOCH 1 - PROGRESS: at 84.24% examples, 925688 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:44: EPOCH 1 - PROGRESS: at 85.20% examples, 925638 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 08:59:45: EPOCH 1 - PROGRESS: at 86.20% examples, 926076 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:46: EPOCH 1 - PROGRESS: at 87.15% examples, 926159 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 08:59:47: EPOCH 1 - PROGRESS: at 88.06% examples, 925994 words/s, in_qsize 11, out_qsize 4\n",
      "INFO - 08:59:48: EPOCH 1 - PROGRESS: at 89.05% examples, 926268 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 08:59:49: EPOCH 1 - PROGRESS: at 90.01% examples, 926243 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:50: EPOCH 1 - PROGRESS: at 90.98% examples, 926565 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:52: EPOCH 1 - PROGRESS: at 91.92% examples, 926632 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:53: EPOCH 1 - PROGRESS: at 92.88% examples, 926962 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:54: EPOCH 1 - PROGRESS: at 93.84% examples, 927219 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 08:59:55: EPOCH 1 - PROGRESS: at 94.80% examples, 927404 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:56: EPOCH 1 - PROGRESS: at 95.73% examples, 927345 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 08:59:57: EPOCH 1 - PROGRESS: at 96.72% examples, 927741 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 08:59:58: EPOCH 1 - PROGRESS: at 97.66% examples, 927753 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 08:59:59: EPOCH 1 - PROGRESS: at 98.63% examples, 927892 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:00: EPOCH 1 - PROGRESS: at 99.55% examples, 927953 words/s, in_qsize 16, out_qsize 3\n",
      "INFO - 09:00:00: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 09:00:00: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:00:00: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:00:00: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:00:00: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:00:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:00:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:00:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:00:00: EPOCH - 1 : training on 128082650 raw words (96932634 effective words) took 104.4s, 928178 effective words/s\n",
      "INFO - 09:00:01: EPOCH 2 - PROGRESS: at 1.06% examples, 891493 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:02: EPOCH 2 - PROGRESS: at 2.14% examples, 903873 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:00:03: EPOCH 2 - PROGRESS: at 3.20% examples, 915259 words/s, in_qsize 16, out_qsize 2\n",
      "INFO - 09:00:04: EPOCH 2 - PROGRESS: at 4.24% examples, 923769 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:05: EPOCH 2 - PROGRESS: at 5.27% examples, 928229 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:06: EPOCH 2 - PROGRESS: at 6.24% examples, 929536 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:07: EPOCH 2 - PROGRESS: at 7.25% examples, 933755 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:08: EPOCH 2 - PROGRESS: at 8.21% examples, 934756 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:09: EPOCH 2 - PROGRESS: at 9.18% examples, 937119 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:10: EPOCH 2 - PROGRESS: at 10.20% examples, 940377 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:11: EPOCH 2 - PROGRESS: at 11.20% examples, 942205 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:12: EPOCH 2 - PROGRESS: at 12.17% examples, 942552 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:13: EPOCH 2 - PROGRESS: at 13.17% examples, 944078 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:14: EPOCH 2 - PROGRESS: at 14.15% examples, 942917 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:15: EPOCH 2 - PROGRESS: at 15.18% examples, 945062 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:16: EPOCH 2 - PROGRESS: at 16.17% examples, 943251 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:17: EPOCH 2 - PROGRESS: at 17.14% examples, 943130 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:18: EPOCH 2 - PROGRESS: at 18.11% examples, 941590 words/s, in_qsize 13, out_qsize 5\n",
      "INFO - 09:00:19: EPOCH 2 - PROGRESS: at 19.13% examples, 943913 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:20: EPOCH 2 - PROGRESS: at 20.07% examples, 943067 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:21: EPOCH 2 - PROGRESS: at 21.05% examples, 943058 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:00:22: EPOCH 2 - PROGRESS: at 22.03% examples, 942872 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:23: EPOCH 2 - PROGRESS: at 22.98% examples, 941987 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:24: EPOCH 2 - PROGRESS: at 23.93% examples, 939490 words/s, in_qsize 16, out_qsize 3\n",
      "INFO - 09:00:25: EPOCH 2 - PROGRESS: at 24.91% examples, 939567 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:26: EPOCH 2 - PROGRESS: at 25.87% examples, 938759 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:27: EPOCH 2 - PROGRESS: at 26.86% examples, 938070 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:28: EPOCH 2 - PROGRESS: at 27.86% examples, 938945 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:29: EPOCH 2 - PROGRESS: at 28.78% examples, 937153 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:30: EPOCH 2 - PROGRESS: at 29.75% examples, 936538 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:31: EPOCH 2 - PROGRESS: at 30.66% examples, 935334 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:00:32: EPOCH 2 - PROGRESS: at 31.62% examples, 934311 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:00:33: EPOCH 2 - PROGRESS: at 32.60% examples, 934190 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:00:34: EPOCH 2 - PROGRESS: at 33.56% examples, 933746 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:35: EPOCH 2 - PROGRESS: at 34.53% examples, 933965 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:36: EPOCH 2 - PROGRESS: at 35.53% examples, 934413 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:37: EPOCH 2 - PROGRESS: at 36.48% examples, 933543 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:38: EPOCH 2 - PROGRESS: at 37.41% examples, 932688 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:39: EPOCH 2 - PROGRESS: at 38.36% examples, 932328 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:40: EPOCH 2 - PROGRESS: at 39.29% examples, 931122 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:41: EPOCH 2 - PROGRESS: at 40.24% examples, 930104 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:42: EPOCH 2 - PROGRESS: at 41.16% examples, 929480 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:43: EPOCH 2 - PROGRESS: at 42.08% examples, 928786 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:00:44: EPOCH 2 - PROGRESS: at 43.00% examples, 927983 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:00:45: EPOCH 2 - PROGRESS: at 43.90% examples, 926750 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 09:00:46: EPOCH 2 - PROGRESS: at 44.82% examples, 925893 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:00:47: EPOCH 2 - PROGRESS: at 45.76% examples, 925623 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:00:48: EPOCH 2 - PROGRESS: at 46.68% examples, 924623 words/s, in_qsize 16, out_qsize 3\n",
      "INFO - 09:00:50: EPOCH 2 - PROGRESS: at 47.64% examples, 924976 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:51: EPOCH 2 - PROGRESS: at 48.60% examples, 924914 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:00:52: EPOCH 2 - PROGRESS: at 49.55% examples, 924786 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:00:53: EPOCH 2 - PROGRESS: at 50.51% examples, 924425 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:54: EPOCH 2 - PROGRESS: at 51.34% examples, 921862 words/s, in_qsize 16, out_qsize 4\n",
      "INFO - 09:00:55: EPOCH 2 - PROGRESS: at 52.27% examples, 921058 words/s, in_qsize 11, out_qsize 4\n",
      "INFO - 09:00:56: EPOCH 2 - PROGRESS: at 53.25% examples, 921693 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:57: EPOCH 2 - PROGRESS: at 54.16% examples, 920941 words/s, in_qsize 15, out_qsize 1\n",
      "INFO - 09:00:58: EPOCH 2 - PROGRESS: at 55.13% examples, 921170 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:00:59: EPOCH 2 - PROGRESS: at 56.09% examples, 921206 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:00: EPOCH 2 - PROGRESS: at 57.06% examples, 921202 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:01: EPOCH 2 - PROGRESS: at 57.96% examples, 920689 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:02: EPOCH 2 - PROGRESS: at 58.87% examples, 920080 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:03: EPOCH 2 - PROGRESS: at 59.81% examples, 919755 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:04: EPOCH 2 - PROGRESS: at 60.75% examples, 919521 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:05: EPOCH 2 - PROGRESS: at 61.71% examples, 919840 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 09:01:06: EPOCH 2 - PROGRESS: at 62.69% examples, 919958 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:01:07: EPOCH 2 - PROGRESS: at 63.66% examples, 920121 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:01:08: EPOCH 2 - PROGRESS: at 64.64% examples, 920175 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:01:09: EPOCH 2 - PROGRESS: at 65.59% examples, 919850 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:10: EPOCH 2 - PROGRESS: at 66.57% examples, 920118 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:11: EPOCH 2 - PROGRESS: at 67.54% examples, 920305 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:12: EPOCH 2 - PROGRESS: at 68.51% examples, 920446 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:13: EPOCH 2 - PROGRESS: at 69.46% examples, 920523 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:14: EPOCH 2 - PROGRESS: at 70.40% examples, 920291 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:15: EPOCH 2 - PROGRESS: at 71.38% examples, 920516 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:16: EPOCH 2 - PROGRESS: at 72.32% examples, 920475 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:17: EPOCH 2 - PROGRESS: at 73.24% examples, 920063 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:18: EPOCH 2 - PROGRESS: at 74.17% examples, 919977 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:19: EPOCH 2 - PROGRESS: at 75.14% examples, 920159 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:20: EPOCH 2 - PROGRESS: at 76.10% examples, 920311 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:21: EPOCH 2 - PROGRESS: at 77.06% examples, 920291 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:22: EPOCH 2 - PROGRESS: at 78.02% examples, 920464 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:23: EPOCH 2 - PROGRESS: at 78.98% examples, 920786 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:24: EPOCH 2 - PROGRESS: at 80.00% examples, 921200 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:25: EPOCH 2 - PROGRESS: at 80.95% examples, 921289 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:01:26: EPOCH 2 - PROGRESS: at 81.93% examples, 921498 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:27: EPOCH 2 - PROGRESS: at 82.89% examples, 921798 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:28: EPOCH 2 - PROGRESS: at 83.84% examples, 921915 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:29: EPOCH 2 - PROGRESS: at 84.81% examples, 922079 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:30: EPOCH 2 - PROGRESS: at 85.78% examples, 922147 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:31: EPOCH 2 - PROGRESS: at 86.76% examples, 922524 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:32: EPOCH 2 - PROGRESS: at 87.70% examples, 922669 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:33: EPOCH 2 - PROGRESS: at 88.71% examples, 923129 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:34: EPOCH 2 - PROGRESS: at 89.73% examples, 923923 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:35: EPOCH 2 - PROGRESS: at 90.69% examples, 924099 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:36: EPOCH 2 - PROGRESS: at 91.68% examples, 924602 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:37: EPOCH 2 - PROGRESS: at 92.64% examples, 925011 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:38: EPOCH 2 - PROGRESS: at 93.60% examples, 925187 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:39: EPOCH 2 - PROGRESS: at 94.54% examples, 925271 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:40: EPOCH 2 - PROGRESS: at 95.50% examples, 925440 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:41: EPOCH 2 - PROGRESS: at 96.44% examples, 925442 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:42: EPOCH 2 - PROGRESS: at 97.38% examples, 925416 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:43: EPOCH 2 - PROGRESS: at 98.33% examples, 925459 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:44: EPOCH 2 - PROGRESS: at 99.26% examples, 925438 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:01:45: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 09:01:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:01:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:01:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:01:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:01:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:01:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:01:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:01:45: EPOCH - 2 : training on 128082650 raw words (96934510 effective words) took 104.7s, 925677 effective words/s\n",
      "INFO - 09:01:46: EPOCH 3 - PROGRESS: at 1.11% examples, 926996 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:01:47: EPOCH 3 - PROGRESS: at 2.23% examples, 941093 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:48: EPOCH 3 - PROGRESS: at 3.28% examples, 934475 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:49: EPOCH 3 - PROGRESS: at 4.34% examples, 939398 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:50: EPOCH 3 - PROGRESS: at 5.34% examples, 937331 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:51: EPOCH 3 - PROGRESS: at 6.36% examples, 945634 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:52: EPOCH 3 - PROGRESS: at 7.36% examples, 947779 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:53: EPOCH 3 - PROGRESS: at 8.33% examples, 944205 words/s, in_qsize 16, out_qsize 2\n",
      "INFO - 09:01:54: EPOCH 3 - PROGRESS: at 9.29% examples, 943572 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:55: EPOCH 3 - PROGRESS: at 10.25% examples, 940920 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:01:56: EPOCH 3 - PROGRESS: at 11.22% examples, 940904 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:01:57: EPOCH 3 - PROGRESS: at 12.18% examples, 940443 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:01:58: EPOCH 3 - PROGRESS: at 13.17% examples, 941803 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:01:59: EPOCH 3 - PROGRESS: at 14.18% examples, 942252 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:00: EPOCH 3 - PROGRESS: at 15.16% examples, 941716 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:01: EPOCH 3 - PROGRESS: at 16.14% examples, 938812 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:02: EPOCH 3 - PROGRESS: at 17.11% examples, 938746 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:03: EPOCH 3 - PROGRESS: at 18.09% examples, 939587 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:04: EPOCH 3 - PROGRESS: at 19.05% examples, 938830 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:05: EPOCH 3 - PROGRESS: at 20.00% examples, 938522 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:06: EPOCH 3 - PROGRESS: at 20.96% examples, 937466 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:07: EPOCH 3 - PROGRESS: at 21.93% examples, 937163 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:08: EPOCH 3 - PROGRESS: at 22.91% examples, 936995 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:09: EPOCH 3 - PROGRESS: at 23.92% examples, 938067 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:10: EPOCH 3 - PROGRESS: at 24.92% examples, 938620 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:11: EPOCH 3 - PROGRESS: at 25.87% examples, 937877 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:02:12: EPOCH 3 - PROGRESS: at 26.86% examples, 937443 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:13: EPOCH 3 - PROGRESS: at 27.82% examples, 937345 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:14: EPOCH 3 - PROGRESS: at 28.78% examples, 936678 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:15: EPOCH 3 - PROGRESS: at 29.77% examples, 936466 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:16: EPOCH 3 - PROGRESS: at 30.67% examples, 934116 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:02:17: EPOCH 3 - PROGRESS: at 31.63% examples, 933545 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:18: EPOCH 3 - PROGRESS: at 32.62% examples, 933858 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:19: EPOCH 3 - PROGRESS: at 33.55% examples, 932555 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:20: EPOCH 3 - PROGRESS: at 34.56% examples, 933709 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:21: EPOCH 3 - PROGRESS: at 35.49% examples, 932574 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:22: EPOCH 3 - PROGRESS: at 36.46% examples, 931797 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:23: EPOCH 3 - PROGRESS: at 37.41% examples, 931819 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:24: EPOCH 3 - PROGRESS: at 38.32% examples, 930694 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:25: EPOCH 3 - PROGRESS: at 39.34% examples, 931356 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:26: EPOCH 3 - PROGRESS: at 40.30% examples, 931076 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:27: EPOCH 3 - PROGRESS: at 41.25% examples, 931099 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:28: EPOCH 3 - PROGRESS: at 42.19% examples, 930949 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:29: EPOCH 3 - PROGRESS: at 43.11% examples, 930029 words/s, in_qsize 15, out_qsize 2\n",
      "INFO - 09:02:30: EPOCH 3 - PROGRESS: at 44.06% examples, 929909 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:31: EPOCH 3 - PROGRESS: at 44.99% examples, 929451 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:32: EPOCH 3 - PROGRESS: at 45.93% examples, 929022 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:02:33: EPOCH 3 - PROGRESS: at 46.89% examples, 929122 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:34: EPOCH 3 - PROGRESS: at 47.80% examples, 928344 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:35: EPOCH 3 - PROGRESS: at 48.74% examples, 928110 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:36: EPOCH 3 - PROGRESS: at 49.70% examples, 927802 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:37: EPOCH 3 - PROGRESS: at 50.68% examples, 927507 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:02:38: EPOCH 3 - PROGRESS: at 51.67% examples, 927520 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:02:39: EPOCH 3 - PROGRESS: at 52.62% examples, 926731 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:40: EPOCH 3 - PROGRESS: at 53.56% examples, 926578 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:41: EPOCH 3 - PROGRESS: at 54.52% examples, 926294 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:02:42: EPOCH 3 - PROGRESS: at 55.49% examples, 926484 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:43: EPOCH 3 - PROGRESS: at 56.44% examples, 926020 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:02:44: EPOCH 3 - PROGRESS: at 57.43% examples, 926540 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:45: EPOCH 3 - PROGRESS: at 58.38% examples, 926225 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:46: EPOCH 3 - PROGRESS: at 59.28% examples, 925531 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:47: EPOCH 3 - PROGRESS: at 60.24% examples, 925693 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:48: EPOCH 3 - PROGRESS: at 61.20% examples, 925817 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:49: EPOCH 3 - PROGRESS: at 62.16% examples, 925807 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:50: EPOCH 3 - PROGRESS: at 63.13% examples, 925913 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:51: EPOCH 3 - PROGRESS: at 64.11% examples, 925960 words/s, in_qsize 16, out_qsize 2\n",
      "INFO - 09:02:52: EPOCH 3 - PROGRESS: at 65.08% examples, 925882 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:02:53: EPOCH 3 - PROGRESS: at 66.03% examples, 925741 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:54: EPOCH 3 - PROGRESS: at 66.98% examples, 925482 words/s, in_qsize 16, out_qsize 2\n",
      "INFO - 09:02:55: EPOCH 3 - PROGRESS: at 67.97% examples, 925731 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:56: EPOCH 3 - PROGRESS: at 68.95% examples, 926262 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:57: EPOCH 3 - PROGRESS: at 69.92% examples, 926280 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:02:58: EPOCH 3 - PROGRESS: at 70.87% examples, 926118 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:02:59: EPOCH 3 - PROGRESS: at 71.86% examples, 926414 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:00: EPOCH 3 - PROGRESS: at 72.83% examples, 926565 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:01: EPOCH 3 - PROGRESS: at 73.75% examples, 926099 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 09:03:02: EPOCH 3 - PROGRESS: at 74.70% examples, 926276 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:04: EPOCH 3 - PROGRESS: at 75.71% examples, 926538 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:03:05: EPOCH 3 - PROGRESS: at 76.70% examples, 927027 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:06: EPOCH 3 - PROGRESS: at 77.66% examples, 927162 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:07: EPOCH 3 - PROGRESS: at 78.62% examples, 927154 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:08: EPOCH 3 - PROGRESS: at 79.63% examples, 927558 words/s, in_qsize 11, out_qsize 4\n",
      "INFO - 09:03:09: EPOCH 3 - PROGRESS: at 80.63% examples, 927725 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:03:10: EPOCH 3 - PROGRESS: at 81.59% examples, 927736 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:11: EPOCH 3 - PROGRESS: at 82.58% examples, 927956 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:12: EPOCH 3 - PROGRESS: at 83.50% examples, 927829 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:13: EPOCH 3 - PROGRESS: at 84.47% examples, 927904 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:03:14: EPOCH 3 - PROGRESS: at 85.44% examples, 927988 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:15: EPOCH 3 - PROGRESS: at 86.41% examples, 928030 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:16: EPOCH 3 - PROGRESS: at 87.37% examples, 928311 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:17: EPOCH 3 - PROGRESS: at 88.32% examples, 928381 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:18: EPOCH 3 - PROGRESS: at 89.29% examples, 928498 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:03:19: EPOCH 3 - PROGRESS: at 90.24% examples, 928587 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:20: EPOCH 3 - PROGRESS: at 91.20% examples, 928828 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:21: EPOCH 3 - PROGRESS: at 92.17% examples, 929125 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:03:22: EPOCH 3 - PROGRESS: at 93.13% examples, 929102 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:03:23: EPOCH 3 - PROGRESS: at 94.06% examples, 929139 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:24: EPOCH 3 - PROGRESS: at 95.00% examples, 929237 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:25: EPOCH 3 - PROGRESS: at 95.99% examples, 929627 words/s, in_qsize 15, out_qsize 1\n",
      "INFO - 09:03:26: EPOCH 3 - PROGRESS: at 96.95% examples, 929670 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:27: EPOCH 3 - PROGRESS: at 97.91% examples, 929893 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:28: EPOCH 3 - PROGRESS: at 98.91% examples, 930219 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:29: EPOCH 3 - PROGRESS: at 99.85% examples, 930341 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:29: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 09:03:29: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:03:29: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:03:29: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:03:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:03:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:03:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:03:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:03:29: EPOCH - 3 : training on 128082650 raw words (96926817 effective words) took 104.2s, 930477 effective words/s\n",
      "INFO - 09:03:30: EPOCH 4 - PROGRESS: at 1.06% examples, 898305 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:31: EPOCH 4 - PROGRESS: at 2.14% examples, 895227 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:32: EPOCH 4 - PROGRESS: at 3.23% examples, 913804 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:33: EPOCH 4 - PROGRESS: at 4.26% examples, 918180 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:34: EPOCH 4 - PROGRESS: at 5.31% examples, 926372 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:35: EPOCH 4 - PROGRESS: at 6.33% examples, 934329 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:36: EPOCH 4 - PROGRESS: at 7.34% examples, 940354 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:37: EPOCH 4 - PROGRESS: at 8.30% examples, 939353 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:38: EPOCH 4 - PROGRESS: at 9.26% examples, 938985 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:39: EPOCH 4 - PROGRESS: at 10.28% examples, 940346 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:40: EPOCH 4 - PROGRESS: at 11.24% examples, 939662 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:41: EPOCH 4 - PROGRESS: at 12.23% examples, 940962 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:42: EPOCH 4 - PROGRESS: at 13.23% examples, 941671 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:43: EPOCH 4 - PROGRESS: at 14.25% examples, 944109 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:44: EPOCH 4 - PROGRESS: at 15.25% examples, 944464 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:45: EPOCH 4 - PROGRESS: at 16.27% examples, 944013 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:03:46: EPOCH 4 - PROGRESS: at 17.23% examples, 943032 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:47: EPOCH 4 - PROGRESS: at 18.20% examples, 942618 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:03:48: EPOCH 4 - PROGRESS: at 19.19% examples, 943047 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:03:49: EPOCH 4 - PROGRESS: at 20.17% examples, 944154 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:50: EPOCH 4 - PROGRESS: at 21.15% examples, 943949 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:03:51: EPOCH 4 - PROGRESS: at 22.14% examples, 944313 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:52: EPOCH 4 - PROGRESS: at 23.13% examples, 945139 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:53: EPOCH 4 - PROGRESS: at 24.12% examples, 944984 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:03:54: EPOCH 4 - PROGRESS: at 25.13% examples, 945059 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:55: EPOCH 4 - PROGRESS: at 26.13% examples, 945665 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:56: EPOCH 4 - PROGRESS: at 27.15% examples, 946515 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:57: EPOCH 4 - PROGRESS: at 28.14% examples, 946510 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:03:58: EPOCH 4 - PROGRESS: at 29.17% examples, 947708 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:03:59: EPOCH 4 - PROGRESS: at 30.16% examples, 948259 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:00: EPOCH 4 - PROGRESS: at 31.15% examples, 948363 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:01: EPOCH 4 - PROGRESS: at 32.09% examples, 946627 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:02: EPOCH 4 - PROGRESS: at 33.11% examples, 947389 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:03: EPOCH 4 - PROGRESS: at 34.13% examples, 948422 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:04: EPOCH 4 - PROGRESS: at 35.10% examples, 948651 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:05: EPOCH 4 - PROGRESS: at 36.07% examples, 947740 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:06: EPOCH 4 - PROGRESS: at 37.05% examples, 947733 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:07: EPOCH 4 - PROGRESS: at 38.02% examples, 947576 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:08: EPOCH 4 - PROGRESS: at 39.01% examples, 947311 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:04:09: EPOCH 4 - PROGRESS: at 39.99% examples, 946921 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:10: EPOCH 4 - PROGRESS: at 40.94% examples, 946803 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:04:11: EPOCH 4 - PROGRESS: at 41.91% examples, 947013 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:12: EPOCH 4 - PROGRESS: at 42.89% examples, 947068 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:13: EPOCH 4 - PROGRESS: at 43.86% examples, 946805 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:14: EPOCH 4 - PROGRESS: at 44.86% examples, 947243 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:04:15: EPOCH 4 - PROGRESS: at 45.83% examples, 947339 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:16: EPOCH 4 - PROGRESS: at 46.76% examples, 946588 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:17: EPOCH 4 - PROGRESS: at 47.76% examples, 947029 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:04:18: EPOCH 4 - PROGRESS: at 48.72% examples, 946822 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:19: EPOCH 4 - PROGRESS: at 49.71% examples, 946870 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:20: EPOCH 4 - PROGRESS: at 50.71% examples, 946680 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:21: EPOCH 4 - PROGRESS: at 51.71% examples, 946646 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:22: EPOCH 4 - PROGRESS: at 52.72% examples, 946942 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:04:23: EPOCH 4 - PROGRESS: at 53.71% examples, 947161 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:24: EPOCH 4 - PROGRESS: at 54.68% examples, 947299 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:25: EPOCH 4 - PROGRESS: at 55.64% examples, 946941 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:26: EPOCH 4 - PROGRESS: at 56.64% examples, 946926 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:04:27: EPOCH 4 - PROGRESS: at 57.64% examples, 947218 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:28: EPOCH 4 - PROGRESS: at 58.62% examples, 947042 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:29: EPOCH 4 - PROGRESS: at 59.61% examples, 947315 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:30: EPOCH 4 - PROGRESS: at 60.58% examples, 947362 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:31: EPOCH 4 - PROGRESS: at 61.54% examples, 946959 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:32: EPOCH 4 - PROGRESS: at 62.55% examples, 947297 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:34: EPOCH 4 - PROGRESS: at 63.54% examples, 947417 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:35: EPOCH 4 - PROGRESS: at 64.56% examples, 947598 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:36: EPOCH 4 - PROGRESS: at 65.57% examples, 947958 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:37: EPOCH 4 - PROGRESS: at 66.57% examples, 948198 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:38: EPOCH 4 - PROGRESS: at 67.56% examples, 948165 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:39: EPOCH 4 - PROGRESS: at 68.57% examples, 948561 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:04:40: EPOCH 4 - PROGRESS: at 69.53% examples, 948267 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:41: EPOCH 4 - PROGRESS: at 70.50% examples, 947977 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:42: EPOCH 4 - PROGRESS: at 71.51% examples, 948081 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:43: EPOCH 4 - PROGRESS: at 72.52% examples, 948435 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:44: EPOCH 4 - PROGRESS: at 73.52% examples, 948846 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:45: EPOCH 4 - PROGRESS: at 74.48% examples, 948632 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:46: EPOCH 4 - PROGRESS: at 75.50% examples, 948815 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:47: EPOCH 4 - PROGRESS: at 76.47% examples, 948556 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:48: EPOCH 4 - PROGRESS: at 77.46% examples, 948580 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:49: EPOCH 4 - PROGRESS: at 78.46% examples, 948519 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:50: EPOCH 4 - PROGRESS: at 79.45% examples, 948733 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:51: EPOCH 4 - PROGRESS: at 80.45% examples, 948923 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:04:52: EPOCH 4 - PROGRESS: at 81.41% examples, 948842 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:53: EPOCH 4 - PROGRESS: at 82.40% examples, 948831 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:54: EPOCH 4 - PROGRESS: at 83.36% examples, 948924 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:55: EPOCH 4 - PROGRESS: at 84.31% examples, 948615 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:04:56: EPOCH 4 - PROGRESS: at 85.31% examples, 948754 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:57: EPOCH 4 - PROGRESS: at 86.30% examples, 948697 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:04:58: EPOCH 4 - PROGRESS: at 87.25% examples, 948583 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:04:59: EPOCH 4 - PROGRESS: at 88.24% examples, 948914 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:00: EPOCH 4 - PROGRESS: at 89.17% examples, 948522 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:01: EPOCH 4 - PROGRESS: at 90.11% examples, 948169 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:02: EPOCH 4 - PROGRESS: at 91.07% examples, 947889 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:03: EPOCH 4 - PROGRESS: at 91.99% examples, 947452 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:04: EPOCH 4 - PROGRESS: at 92.94% examples, 947326 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:05: EPOCH 4 - PROGRESS: at 93.87% examples, 947116 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:06: EPOCH 4 - PROGRESS: at 94.82% examples, 947188 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:07: EPOCH 4 - PROGRESS: at 95.80% examples, 947451 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:08: EPOCH 4 - PROGRESS: at 96.80% examples, 947761 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:09: EPOCH 4 - PROGRESS: at 97.71% examples, 947364 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:10: EPOCH 4 - PROGRESS: at 98.66% examples, 947240 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:11: EPOCH 4 - PROGRESS: at 99.61% examples, 947333 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:11: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 09:05:11: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:05:11: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:05:11: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:05:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:05:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:05:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:05:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:05:11: EPOCH - 4 : training on 128082650 raw words (96931336 effective words) took 102.3s, 947490 effective words/s\n",
      "INFO - 09:05:12: EPOCH 5 - PROGRESS: at 1.06% examples, 887921 words/s, in_qsize 15, out_qsize 3\n",
      "INFO - 09:05:13: EPOCH 5 - PROGRESS: at 2.20% examples, 932244 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:14: EPOCH 5 - PROGRESS: at 3.29% examples, 944822 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:15: EPOCH 5 - PROGRESS: at 4.36% examples, 948795 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:05:16: EPOCH 5 - PROGRESS: at 5.38% examples, 949938 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:17: EPOCH 5 - PROGRESS: at 6.36% examples, 947558 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:18: EPOCH 5 - PROGRESS: at 7.34% examples, 949315 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:19: EPOCH 5 - PROGRESS: at 8.30% examples, 947389 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:20: EPOCH 5 - PROGRESS: at 9.29% examples, 949388 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:05:21: EPOCH 5 - PROGRESS: at 10.30% examples, 951541 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:22: EPOCH 5 - PROGRESS: at 11.26% examples, 950027 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:23: EPOCH 5 - PROGRESS: at 12.23% examples, 948599 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 09:05:24: EPOCH 5 - PROGRESS: at 13.24% examples, 950910 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:25: EPOCH 5 - PROGRESS: at 14.23% examples, 949624 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:26: EPOCH 5 - PROGRESS: at 15.24% examples, 950570 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:27: EPOCH 5 - PROGRESS: at 16.25% examples, 949773 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:28: EPOCH 5 - PROGRESS: at 17.22% examples, 949382 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:29: EPOCH 5 - PROGRESS: at 18.18% examples, 948250 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:30: EPOCH 5 - PROGRESS: at 19.18% examples, 948748 words/s, in_qsize 14, out_qsize 4\n",
      "INFO - 09:05:31: EPOCH 5 - PROGRESS: at 20.16% examples, 949392 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:32: EPOCH 5 - PROGRESS: at 21.15% examples, 949762 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:33: EPOCH 5 - PROGRESS: at 22.12% examples, 948824 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:34: EPOCH 5 - PROGRESS: at 23.10% examples, 948765 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:35: EPOCH 5 - PROGRESS: at 24.04% examples, 946813 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:05:36: EPOCH 5 - PROGRESS: at 25.06% examples, 947294 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:37: EPOCH 5 - PROGRESS: at 26.04% examples, 947449 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:38: EPOCH 5 - PROGRESS: at 27.06% examples, 947996 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:39: EPOCH 5 - PROGRESS: at 28.05% examples, 948100 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:40: EPOCH 5 - PROGRESS: at 29.02% examples, 946805 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:41: EPOCH 5 - PROGRESS: at 30.05% examples, 947668 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:42: EPOCH 5 - PROGRESS: at 31.02% examples, 947701 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:43: EPOCH 5 - PROGRESS: at 31.94% examples, 945671 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:44: EPOCH 5 - PROGRESS: at 32.93% examples, 945335 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:45: EPOCH 5 - PROGRESS: at 33.91% examples, 945177 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:46: EPOCH 5 - PROGRESS: at 34.87% examples, 945023 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:48: EPOCH 5 - PROGRESS: at 35.87% examples, 944899 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:49: EPOCH 5 - PROGRESS: at 36.80% examples, 942927 words/s, in_qsize 13, out_qsize 6\n",
      "INFO - 09:05:50: EPOCH 5 - PROGRESS: at 37.77% examples, 942819 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:51: EPOCH 5 - PROGRESS: at 38.75% examples, 942635 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:52: EPOCH 5 - PROGRESS: at 39.74% examples, 942496 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:53: EPOCH 5 - PROGRESS: at 40.67% examples, 941799 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:54: EPOCH 5 - PROGRESS: at 41.61% examples, 940867 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:55: EPOCH 5 - PROGRESS: at 42.58% examples, 941026 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:56: EPOCH 5 - PROGRESS: at 43.55% examples, 940873 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:05:57: EPOCH 5 - PROGRESS: at 44.54% examples, 940672 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:05:58: EPOCH 5 - PROGRESS: at 45.47% examples, 940397 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:05:59: EPOCH 5 - PROGRESS: at 46.42% examples, 940464 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:00: EPOCH 5 - PROGRESS: at 47.35% examples, 939566 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:06:01: EPOCH 5 - PROGRESS: at 48.31% examples, 939566 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:02: EPOCH 5 - PROGRESS: at 49.21% examples, 938083 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:03: EPOCH 5 - PROGRESS: at 50.20% examples, 938263 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:04: EPOCH 5 - PROGRESS: at 51.16% examples, 937437 words/s, in_qsize 15, out_qsize 1\n",
      "INFO - 09:06:05: EPOCH 5 - PROGRESS: at 52.13% examples, 936976 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:06: EPOCH 5 - PROGRESS: at 53.11% examples, 937038 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:07: EPOCH 5 - PROGRESS: at 54.03% examples, 936361 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:08: EPOCH 5 - PROGRESS: at 54.95% examples, 935759 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:09: EPOCH 5 - PROGRESS: at 55.88% examples, 934849 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:10: EPOCH 5 - PROGRESS: at 56.82% examples, 934225 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:11: EPOCH 5 - PROGRESS: at 57.72% examples, 933399 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:12: EPOCH 5 - PROGRESS: at 58.63% examples, 932232 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:06:13: EPOCH 5 - PROGRESS: at 59.56% examples, 931665 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:14: EPOCH 5 - PROGRESS: at 60.44% examples, 930548 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:06:15: EPOCH 5 - PROGRESS: at 61.38% examples, 930064 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:16: EPOCH 5 - PROGRESS: at 62.29% examples, 929322 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:06:17: EPOCH 5 - PROGRESS: at 63.21% examples, 928808 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:18: EPOCH 5 - PROGRESS: at 64.19% examples, 928663 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:19: EPOCH 5 - PROGRESS: at 65.17% examples, 928581 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:20: EPOCH 5 - PROGRESS: at 66.09% examples, 927900 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:21: EPOCH 5 - PROGRESS: at 67.05% examples, 927815 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 09:06:22: EPOCH 5 - PROGRESS: at 68.06% examples, 928088 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:23: EPOCH 5 - PROGRESS: at 69.04% examples, 928275 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:24: EPOCH 5 - PROGRESS: at 70.00% examples, 928153 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:25: EPOCH 5 - PROGRESS: at 70.95% examples, 928021 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:26: EPOCH 5 - PROGRESS: at 71.94% examples, 928430 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:27: EPOCH 5 - PROGRESS: at 72.86% examples, 927950 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:28: EPOCH 5 - PROGRESS: at 73.79% examples, 927413 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:06:29: EPOCH 5 - PROGRESS: at 74.71% examples, 927056 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:30: EPOCH 5 - PROGRESS: at 75.64% examples, 926414 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:31: EPOCH 5 - PROGRESS: at 76.55% examples, 925673 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:32: EPOCH 5 - PROGRESS: at 77.46% examples, 925085 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:33: EPOCH 5 - PROGRESS: at 78.34% examples, 924217 words/s, in_qsize 16, out_qsize 3\n",
      "INFO - 09:06:34: EPOCH 5 - PROGRESS: at 79.27% examples, 924026 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:06:35: EPOCH 5 - PROGRESS: at 80.21% examples, 923657 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:36: EPOCH 5 - PROGRESS: at 81.11% examples, 923040 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:37: EPOCH 5 - PROGRESS: at 82.02% examples, 922600 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:38: EPOCH 5 - PROGRESS: at 82.94% examples, 922269 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:06:39: EPOCH 5 - PROGRESS: at 83.83% examples, 921555 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:06:40: EPOCH 5 - PROGRESS: at 84.76% examples, 921290 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:41: EPOCH 5 - PROGRESS: at 85.70% examples, 921116 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:42: EPOCH 5 - PROGRESS: at 86.66% examples, 921278 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:06:43: EPOCH 5 - PROGRESS: at 87.61% examples, 921607 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:44: EPOCH 5 - PROGRESS: at 88.56% examples, 921704 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:45: EPOCH 5 - PROGRESS: at 89.55% examples, 922013 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 09:06:46: EPOCH 5 - PROGRESS: at 90.48% examples, 921889 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:47: EPOCH 5 - PROGRESS: at 91.42% examples, 922004 words/s, in_qsize 16, out_qsize 4\n",
      "INFO - 09:06:48: EPOCH 5 - PROGRESS: at 92.42% examples, 922536 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:49: EPOCH 5 - PROGRESS: at 93.35% examples, 922402 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 09:06:50: EPOCH 5 - PROGRESS: at 94.30% examples, 922469 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:51: EPOCH 5 - PROGRESS: at 95.24% examples, 922591 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 09:06:52: EPOCH 5 - PROGRESS: at 96.15% examples, 922486 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:53: EPOCH 5 - PROGRESS: at 97.15% examples, 923054 words/s, in_qsize 13, out_qsize 2\n",
      "INFO - 09:06:54: EPOCH 5 - PROGRESS: at 98.12% examples, 923317 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 09:06:55: EPOCH 5 - PROGRESS: at 99.07% examples, 923394 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 09:06:56: EPOCH 5 - PROGRESS: at 99.96% examples, 923187 words/s, in_qsize 7, out_qsize 1\n",
      "INFO - 09:06:56: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 09:06:56: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:06:56: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:06:56: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:06:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:06:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:06:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:06:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:06:56: EPOCH - 5 : training on 128082650 raw words (96928994 effective words) took 105.0s, 923234 effective words/s\n",
      "INFO - 09:06:56: training on a 640413250 raw words (484654291 effective words) took 520.6s, 930880 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49min 15s, sys: 15.4 s, total: 49min 30s\n",
      "Wall time: 17min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "#https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "\n",
    "#size: The number of dimensions of the embeddings and the default is 100.\n",
    "#window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "#min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "#workers: The number of partitions during training and the default workers is 3.\n",
    "#sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "\n",
    "model1 = Word2Vec(corpus, min_count=2,size= 300,workers=8, window =5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747920"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model1.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary of one million seven hundred forty-seven thousand nine hundred twenty words is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50918806e+00 -1.85878778e+00  1.61693120e+00  2.58832645e+00\n",
      " -3.20206618e+00 -4.03099149e-01 -2.53829193e+00  2.51184225e+00\n",
      "  6.77844882e-01 -3.08668882e-01  6.47953868e-01 -7.95691162e-02\n",
      " -1.22440958e+00  8.85386392e-02  2.76785374e+00  2.49264717e+00\n",
      " -2.98029542e+00 -1.81757462e+00 -2.50846952e-01  1.28881061e+00\n",
      " -4.94186550e-01  1.06023125e-01 -2.00635102e-03 -3.80804509e-01\n",
      " -1.69312894e+00  1.68917775e+00 -1.70153886e-01  1.94352436e+00\n",
      "  5.18117785e-01  2.14713526e+00 -2.00733495e+00  8.20280492e-01\n",
      "  3.09260297e+00  2.60101169e-01  3.47028518e+00  6.12944543e-01\n",
      "  5.75977445e-01 -9.98596489e-01  1.65202844e+00  1.21345162e+00\n",
      " -1.18380710e-01  1.46645471e-01 -1.73597023e-01 -6.14722490e-01\n",
      "  1.12078631e+00 -5.17284930e-01 -1.31699252e+00  1.49792254e+00\n",
      "  1.71648669e+00 -3.87544107e+00 -7.86019683e-01  9.54831541e-01\n",
      " -8.34102690e-01  2.59639645e+00 -3.84834260e-01 -1.28887141e+00\n",
      " -1.10986829e+00 -7.00336695e-02  2.26638699e+00 -1.47424843e-02\n",
      "  1.12565482e+00  6.24476254e-01  4.80222642e-01  1.63982463e+00\n",
      "  4.49823588e-01 -2.88614213e-01 -8.91776502e-01 -3.60740662e-01\n",
      "  4.68773656e-02  4.50385523e+00 -2.22938776e+00 -1.04055667e+00\n",
      " -7.24030137e-01  1.25024521e+00 -1.41373217e+00 -3.41649175e+00\n",
      " -3.65563774e+00 -1.14100683e+00 -1.60881296e-01 -9.96015668e-02\n",
      "  4.87265214e-02  2.07964182e+00 -2.13676739e+00  6.38397932e-02\n",
      "  2.74965024e+00  1.63148332e+00  9.35378000e-02  2.31456828e+00\n",
      "  1.03974000e-01  8.71972442e-01 -1.10337114e+00 -1.66724831e-01\n",
      " -1.37092733e+00 -1.95717108e+00  1.84679580e+00  3.03287292e+00\n",
      " -6.97667122e-01  1.10825336e+00 -8.30785632e-01 -1.62025750e-01\n",
      "  1.79161236e-01  1.16937590e+00 -4.25921869e+00  2.14352059e+00\n",
      "  8.78223598e-01  1.94402552e+00  2.40397438e-01 -2.14923882e+00\n",
      " -1.16890483e-01 -7.64528692e-01  1.06434762e+00  1.19254220e+00\n",
      "  1.42379332e+00  1.51131821e+00  2.62911111e-01 -3.97657186e-01\n",
      " -4.29979563e-01  2.09925503e-01  1.08507395e+00 -1.65678465e+00\n",
      "  3.82800072e-01  1.05317616e+00  1.18082762e+00  1.11162090e+00\n",
      " -2.98376060e+00  1.19775248e+00  8.83845091e-01 -2.07643732e-01\n",
      " -4.03076410e-01 -2.53150970e-01 -7.70892322e-01  2.05785632e+00\n",
      "  3.38073403e-01  8.52449059e-01 -1.98733282e+00  1.82687700e-01\n",
      "  1.12113750e+00  1.37616324e+00  1.52297735e+00 -8.62816215e-01\n",
      " -2.68648386e+00  1.04245651e+00 -1.81014943e+00  1.32596946e+00\n",
      " -2.44002283e-01 -7.10150182e-01  5.94324887e-01 -1.70605791e+00\n",
      " -3.84651035e-01  1.94113207e+00 -2.78793238e-02  1.70179987e+00\n",
      "  1.11911309e+00 -3.73877573e+00 -2.67020106e+00 -1.89370143e+00\n",
      " -5.43052554e-01  1.25787959e-01  6.14822805e-01 -2.53089845e-01\n",
      " -6.99529350e-01 -3.50602418e-01 -9.26692665e-01 -5.74739218e-01\n",
      " -9.42232311e-01 -1.84772432e+00  2.08575463e+00 -1.87695861e+00\n",
      "  4.46355850e-01  4.56413358e-01  1.25666261e+00 -8.09950590e-01\n",
      "  8.01261008e-01 -7.39340663e-01  8.86886239e-01  1.48046005e+00\n",
      "  1.60343051e+00  1.91430938e+00  1.66265523e+00 -1.54916322e+00\n",
      "  6.36362255e-01 -3.37435794e+00 -1.31930804e+00  1.79658696e-01\n",
      "  1.28412569e+00 -4.52192068e+00  3.31561714e-01  1.55775800e-01\n",
      "  1.52737141e+00 -1.43487990e+00  3.66209292e+00  1.19510639e+00\n",
      " -1.95814526e+00  6.65707827e-01 -4.23393488e-01 -1.42103195e+00\n",
      " -1.41491443e-01 -2.44922876e+00  2.08008385e+00 -5.94857872e-01\n",
      "  1.89585268e+00  1.31582153e+00  1.46351659e+00  4.75759119e-01\n",
      "  1.03188539e+00  2.95029879e+00  3.04229498e+00  8.02671075e-01\n",
      " -2.24603009e+00  2.91112542e+00 -3.96087432e+00  1.52148354e+00\n",
      " -2.15302825e+00 -2.45719481e+00 -1.91773963e+00 -1.76827657e+00\n",
      "  2.28126332e-01  1.24629569e+00  1.04451060e+00 -1.13901389e+00\n",
      "  3.15760896e-02 -4.15049523e-01 -1.76562750e+00  2.67320943e+00\n",
      "  5.77530146e-01  1.10074468e-01 -4.02575612e-01  1.75393313e-01\n",
      " -9.21574831e-01  4.81583923e-01  1.09387648e+00  1.74468434e+00\n",
      "  4.56678897e-01 -1.39833197e-01 -8.46182883e-01 -1.99124157e+00\n",
      " -1.39036632e+00  3.58860373e-01  2.68313617e-01 -4.88484859e-01\n",
      " -7.07758814e-02 -3.35878193e-01 -1.53513896e+00  9.50106323e-01\n",
      "  2.51796532e+00  4.47084808e+00  2.05243659e+00 -2.36156568e-01\n",
      " -2.01886964e+00 -2.06591296e+00 -6.60639226e-01 -9.41829562e-01\n",
      "  9.44114327e-02 -7.36499310e-01  5.51541805e-01  4.26508284e+00\n",
      "  1.52540341e-01 -2.08912039e+00 -1.36262679e+00  2.43031919e-01\n",
      " -1.83185056e-01 -6.36352003e-02 -2.43214905e-01  3.62979248e-02\n",
      "  1.48467541e+00 -1.33908451e+00  4.42809522e-01  3.48835200e-01\n",
      "  2.68246651e+00  2.15914583e+00 -1.43331778e+00  8.33413064e-01\n",
      "  1.09002888e+00  2.06282902e+00 -6.19456828e-01 -9.83381689e-01\n",
      " -2.88905907e+00 -6.51988447e-01 -2.97514057e+00  1.85054421e+00\n",
      "  1.67937532e-01 -1.50344944e+00 -2.33912677e-01  1.04310560e+00\n",
      " -1.91225755e+00 -9.42611396e-01  9.11271632e-01  2.83735543e-02\n",
      "  1.44566804e-01  1.38943291e+00 -2.36250296e-01 -8.44423354e-01\n",
      " -1.37551081e+00  5.81149757e-01  1.85385096e+00  1.20393567e-01\n",
      "  4.62070107e-01 -1.20760024e+00 -8.17225218e-01 -1.29554415e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "print(model1['python'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:06:56: saving Word2Vec object under model/custom_trained_w2v/word2vec_v2.model, separately None\n",
      "INFO - 09:06:56: storing np array 'syn1neg' to model/custom_trained_w2v/word2vec_v2.model.trainables.syn1neg.npy\n",
      "INFO - 09:07:14: storing np array 'vectors' to model/custom_trained_w2v/word2vec_v2.model.wv.vectors.npy\n"
     ]
    }
   ],
   "source": [
    "model1.save(\"model/custom_trained_w2v/word2vec_v2.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load custom trained w2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:10:17: loading Word2Vec object from model/custom_trained_w2v/word2vec_v2.model\n",
      "INFO - 09:10:23: loading trainables recursively from model/custom_trained_w2v/word2vec_v2.model.trainables.* with mmap=None\n",
      "INFO - 09:10:23: loading syn1neg from model/custom_trained_w2v/word2vec_v2.model.trainables.syn1neg.npy with mmap=None\n",
      "INFO - 09:10:40: loading vocabulary recursively from model/custom_trained_w2v/word2vec_v2.model.vocabulary.* with mmap=None\n",
      "INFO - 09:10:40: loading wv recursively from model/custom_trained_w2v/word2vec_v2.model.wv.* with mmap=None\n",
      "INFO - 09:10:40: loading vectors from model/custom_trained_w2v/word2vec_v2.model.wv.vectors.npy with mmap=None\n",
      "INFO - 09:10:56: setting ignored attribute vectors_norm to None\n",
      "INFO - 09:10:56: setting ignored attribute cum_table to None\n",
      "INFO - 09:10:56: loaded model/custom_trained_w2v/word2vec_v2.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 3.3 s, total: 13.9 s\n",
      "Wall time: 44.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loaded_model = Word2Vec.load(\"model/custom_trained_w2v/word2vec_v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dictionary,', 0.8161787986755371),\n",
       " ('dict', 0.7764616012573242),\n",
       " ('hashtable', 0.7649470567703247),\n",
       " ('dictionary.', 0.7405421733856201),\n",
       " ('hashmap', 0.7313495874404907),\n",
       " ('dictionary?', 0.7255147695541382),\n",
       " ('dictionaries', 0.7158240079879761),\n",
       " ('tuple', 0.6831574440002441),\n",
       " ('collection', 0.6677265167236328),\n",
       " ('hashset', 0.6446368098258972),\n",
       " ('keyvaluepair', 0.636683464050293),\n",
       " ('array', 0.6243866086006165),\n",
       " ('dictionary:', 0.6164132356643677),\n",
       " ('sorteddictionary', 0.6145343780517578),\n",
       " ('dictionaries,', 0.6042093634605408)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html\n",
    "# method uses cosine similarity\n",
    "loaded_model.wv.most_similar('dictionary', topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Its amazing to see how similar words include soreteddictionary,hashtables,keyvaluepair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perl', 0.758644700050354),\n",
       " ('python,', 0.7367944121360779),\n",
       " ('python.', 0.7332587242126465),\n",
       " ('python?', 0.6904996633529663),\n",
       " ('ruby', 0.6825644969940186),\n",
       " ('haskell', 0.6816902756690979),\n",
       " (\"python's\", 0.6795947551727295),\n",
       " ('jython', 0.6729694604873657),\n",
       " ('tcl', 0.6706594228744507),\n",
       " ('bash', 0.6340200901031494)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.wv.most_similar('python', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar words included:-\n",
    "- Jython (Java implementation of python)\n",
    "- cython (c extension for python)\n",
    "- ipython (browser based notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30442676]]\n",
      "[[0.5608]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#print(cosine_similarity(np.array(loaded_model['python']).reshape(1,-1), np.array(loaded_model['pandas']).reshape(1,-1)))\n",
    "print(cosine_similarity(np.array(loaded_model['python']).reshape(1,-1), np.array(loaded_model['dictionary']).reshape(1,-1)))\n",
    "print(cosine_similarity(np.array(loaded_model['html']).reshape(1,-1), np.array(loaded_model['css']).reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pre-trained glove and w2v model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# stanfords pre-trained 300 dim glove vectors\n",
    "\n",
    "#!wget --header=\"Host: doc-0c-0k-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://drive.google.com/\" --header=\"Cookie: AUTH_9a0rnn520492vdm9l67ltu9t8cpotig0_nonce=0d2fulvulvjhm\" --header=\"Connection: keep-alive\" \"https://doc-0c-0k-docs.googleusercontent.com/docs/securesc/ssq6alpsc1e8bsr56d67t9bugoing32d/vqpvemgir6vnf7k8mnpih7l7jhjo6bmn/1636535850000/00484516897554883881/10996415371724027994/1lDca_ge-GYO0iQ6_XDLWePQFMdAA2b8f?e=download&authuser=0&nonce=0d2fulvulvjhm&user=10996415371724027994&hash=rcthh8u4u0rerd2j89a2l1tgd5cs33ff\" -c -O 'glove_vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html\n",
    "# Pre-trained w2v trained on google news dataset\n",
    "#!wget --header=\"Host: doc-10-5k-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://drive.google.com/\" --header=\"Cookie: AUTH_9a0rnn520492vdm9l67ltu9t8cpotig0_nonce=oencppjtu4ofo\" --header=\"Connection: keep-alive\" \"https://doc-10-5k-docs.googleusercontent.com/docs/securesc/ssq6alpsc1e8bsr56d67t9bugoing32d/545v2gosgsltblk4h005n0bpitncgc2d/1636537125000/06848720943842814915/10996415371724027994/0B7XkCwpI5KDYNlNUTTlSS21pQmM?e=download&authuser=0&nonce=oencppjtu4ofo&user=10996415371724027994&hash=m9u0ofet090md869bufn7nab4bitv4pj\" -c -O 'GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:11:54: loading Word2Vec object from model/custom_trained_w2v/word2vec_v2.model\n",
      "INFO - 09:12:03: loading trainables recursively from model/custom_trained_w2v/word2vec_v2.model.trainables.* with mmap=None\n",
      "INFO - 09:12:03: loading syn1neg from model/custom_trained_w2v/word2vec_v2.model.trainables.syn1neg.npy with mmap=None\n",
      "INFO - 09:12:04: loading vocabulary recursively from model/custom_trained_w2v/word2vec_v2.model.vocabulary.* with mmap=None\n",
      "INFO - 09:12:04: loading wv recursively from model/custom_trained_w2v/word2vec_v2.model.wv.* with mmap=None\n",
      "INFO - 09:12:04: loading vectors from model/custom_trained_w2v/word2vec_v2.model.wv.vectors.npy with mmap=None\n",
      "INFO - 09:12:05: setting ignored attribute vectors_norm to None\n",
      "INFO - 09:12:05: setting ignored attribute cum_table to None\n",
      "INFO - 09:12:05: loaded model/custom_trained_w2v/word2vec_v2.model\n",
      "INFO - 09:12:12: loading projection weights from model/GoogleNews-vectors-negative300.bin.gz\n",
      "INFO - 09:13:15: loaded (3000000, 300) matrix from model/GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "\n",
    "\n",
    "#please use below code to load glove vectors\n",
    "with open('model/glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words = set(model.keys())\n",
    "    \n",
    "loaded_model = Word2Vec.load(\"model/custom_trained_w2v/word2vec_v2.model\")\n",
    "\n",
    "word2vec_path = 'model/GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v_pretrained_model = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Trained W2v model\n",
      "Similarity between python and dictionary:  [[0.30442676]]\n",
      "Similarity between html and css:  [[0.5608]]\n",
      "Similarity between dataframe and series:  [[0.43597504]]\n",
      "Similarity between php and javascript:  [[0.6253061]]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Pre-Trained Glove model: \n",
      "Similarity between python and dictionary:  [[0.28257843]]\n",
      "Similarity between html and css:  [[0.73995874]]\n",
      "Similarity between python and series:  [[0.22612679]]\n",
      "Similarity between php and javascript:  [[0.64708991]]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Pre-Trained W2V model: \n",
      "Similarity between python and dictionary:  [[0.13850798]]\n",
      "Similarity between html and css:  [[0.5491108]]\n",
      "Similarity between python and series:  [[-0.02696471]]\n",
      "Similarity between php and javascript:  [[0.544561]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom Trained W2v model\")\n",
    "#print(cosine_similarity(np.array(loaded_model['python']).reshape(1,-1), np.array(loaded_model['pandas']).reshape(1,-1)))\n",
    "print(\"Similarity between python and dictionary: \",\n",
    "      cosine_similarity(np.array(loaded_model['python']).reshape(1,-1), np.array(loaded_model['dictionary']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between html and css: \",\n",
    "      cosine_similarity(np.array(loaded_model['html']).reshape(1,-1), np.array(loaded_model['css']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between dataframe and series: \",\n",
    "      cosine_similarity(np.array(loaded_model['dataframe']).reshape(1,-1), np.array(loaded_model['series']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between php and javascript: \",\n",
    "      cosine_similarity(np.array(loaded_model['php']).reshape(1,-1), np.array(loaded_model['javascript']).reshape(1,-1)))\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Pre-Trained Glove model: \")\n",
    "\n",
    "#print(cosine_similarity(np.array(model['python']).reshape(1,-1), np.array(model['pandas']).reshape(1,-1)))\n",
    "print(\"Similarity between python and dictionary: \",\n",
    "      cosine_similarity(np.array(model['python']).reshape(1,-1), np.array(model['dictionary']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between html and css: \",\n",
    "      cosine_similarity(np.array(model['html']).reshape(1,-1), np.array(model['css']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between python and series: \",\n",
    "      cosine_similarity(np.array(model['python']).reshape(1,-1), np.array(model['series']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between php and javascript: \",\n",
    "      cosine_similarity(np.array(model['php']).reshape(1,-1), np.array(model['javascript']).reshape(1,-1)))\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Pre-Trained W2V model: \")\n",
    "\n",
    "#print(cosine_similarity(np.array(w2v_pretrained_model['python']).reshape(1,-1), np.array(w2v_pretrained_model['pandas']).reshape(1,-1)))\n",
    "print(\"Similarity between python and dictionary: \",\n",
    "      cosine_similarity(np.array(w2v_pretrained_model['python']).reshape(1,-1), np.array(w2v_pretrained_model['dictionary']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between html and css: \",\n",
    "      cosine_similarity(np.array(w2v_pretrained_model['html']).reshape(1,-1), np.array(w2v_pretrained_model['css']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between python and series: \",\n",
    "      cosine_similarity(np.array(w2v_pretrained_model['python']).reshape(1,-1), np.array(w2v_pretrained_model['series']).reshape(1,-1)))\n",
    "\n",
    "print(\"Similarity between php and javascript: \",\n",
    "      cosine_similarity(np.array(w2v_pretrained_model['php']).reshape(1,-1), np.array(w2v_pretrained_model['javascript']).reshape(1,-1)))\n",
    "\n",
    "# pandas , apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cosine_similarity(np.array(loaded_model['scipy']).reshape(1,-1), np.array(loaded_model['numpy']).reshape(1,-1)))\n",
    "# print(cosine_similarity(np.array(model['scipy']).reshape(1,-1), np.array(model['numpy']).reshape(1,-1)))\n",
    "# print(cosine_similarity(np.array(w2v_pretrained_model['scipy']).reshape(1,-1), np.array(w2v_pretrained_model['numpy']).reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cosine_similarity(np.array(loaded_model['flask']).reshape(1,-1), np.array(loaded_model['django']).reshape(1,-1)))\n",
    "# print(cosine_similarity(np.array(model['flask']).reshape(1,-1), np.array(model['django']).reshape(1,-1)))\n",
    "# print(cosine_similarity(np.array(w2v_pretrained_model['flask']).reshape(1,-1), np.array(w2v_pretrained_model['django']).reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7082475]]\n",
      "[[0.23233346]]\n",
      "[[0.12214837]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(np.array(loaded_model['iphone']).reshape(1,-1), np.array(loaded_model['cocoa']).reshape(1,-1)))\n",
    "print(cosine_similarity(np.array(model['iphone']).reshape(1,-1), np.array(model['cocoa']).reshape(1,-1)))\n",
    "print(cosine_similarity(np.array(w2v_pretrained_model['iphone']).reshape(1,-1), np.array(w2v_pretrained_model['cocoa']).reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences:-\n",
    "\n",
    "- For similarity between 'python' and 'dictionary' other 2 models might be referring to python snake while our model is referring to technical terms 'python' programming language though it didnt captured much similarity between two.\n",
    "- Glove model seems to be working well compared to pretrained w2v as it is trained on wikipedia dataset so it might contains technical similarity as well.\n",
    "- But Pre-trained models including glove didn't contain word such as 'dataframe', 'django', 'scipy' in its vocab.\n",
    "- For words like 'iphone' and 'cocoa' (Apple's application-development framework for macOS) custom model gave highest similarity among all.\n",
    "- Custom trained w2v model is best choice for our problem statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cosine_similarity(loaded_model.wv.__getitem__(['python']), loaded_model.wv.__getitem__(['pandas']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see performance of custom trained w2v on our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(stopwords.words('english'))\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:21:50: loading Word2Vec object from model/custom_trained_w2v/word2vec_v2.model\n",
      "INFO - 09:21:59: loading trainables recursively from model/custom_trained_w2v/word2vec_v2.model.trainables.* with mmap=None\n",
      "INFO - 09:21:59: loading syn1neg from model/custom_trained_w2v/word2vec_v2.model.trainables.syn1neg.npy with mmap=None\n",
      "INFO - 09:22:00: loading vocabulary recursively from model/custom_trained_w2v/word2vec_v2.model.vocabulary.* with mmap=None\n",
      "INFO - 09:22:00: loading wv recursively from model/custom_trained_w2v/word2vec_v2.model.wv.* with mmap=None\n",
      "INFO - 09:22:00: loading vectors from model/custom_trained_w2v/word2vec_v2.model.wv.vectors.npy with mmap=None\n",
      "INFO - 09:22:01: setting ignored attribute vectors_norm to None\n",
      "INFO - 09:22:01: setting ignored attribute cum_table to None\n",
      "INFO - 09:22:01: loaded model/custom_trained_w2v/word2vec_v2.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1747920"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Word2Vec.load(\"model/custom_trained_w2v/word2vec_v2.model\")\n",
    "\n",
    "customtrained_w2v_words = list(loaded_model.wv.vocab)\n",
    "len(customtrained_w2v_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PostTypeId', 'Question_Id', 'Title', 'Tags', 'AnswerCount',\n",
       "       'ViewCount', 'Body', 'Score', 'Answer_corpus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Final_df.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [04:42<00:00, 3533.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 36s, sys: 7.7 s, total: 4min 44s\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['Cleaned_Titles'] = df['Title'].progress_apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150219"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(sentence):\n",
    "    '''Get 300 dim word embedding for each word from custom trained w2v model.\n",
    "       Avg word embedding to create sentence embedding\n",
    "       \n",
    "       Function accepts only one parameter - sentence (text input)\n",
    "       returns - 300 dim sentence embedding'''\n",
    "    \n",
    "    custom_w2v = []\n",
    "    for word in sentence.split():\n",
    "        if (word not in stopwords):\n",
    "            try:\n",
    "                custom_w2v.append(loaded_model[word]) #keyerror\n",
    "            except:\n",
    "                pass\n",
    "         \n",
    "    avg_w2v = np.array(custom_w2v).mean(axis=0)\n",
    "    return avg_w2v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000000 [00:00<?, ?it/s]/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  0%|          | 986/1000000 [00:00<01:41, 9859.52it/s]/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: RuntimeWarning: Mean of empty slice.\n",
      "100%|██████████| 1000000/1000000 [01:30<00:00, 11086.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 853 ms, total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['Sentence_Embedding'] = df['Cleaned_Titles'].progress_apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      "PostTypeId            1000000 non-null int64\n",
      "Question_Id           1000000 non-null int64\n",
      "Title                 1000000 non-null object\n",
      "Tags                  1000000 non-null object\n",
      "AnswerCount           1000000 non-null int64\n",
      "ViewCount             1000000 non-null int64\n",
      "Body                  1000000 non-null object\n",
      "Score                 1000000 non-null int64\n",
      "Answer_corpus         1000000 non-null object\n",
      "Cleaned_Titles        1000000 non-null object\n",
      "Sentence_Embedding    999581 non-null object\n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 83.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 999581 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      "PostTypeId            999581 non-null int64\n",
      "Question_Id           999581 non-null int64\n",
      "Title                 999581 non-null object\n",
      "Tags                  999581 non-null object\n",
      "AnswerCount           999581 non-null int64\n",
      "ViewCount             999581 non-null int64\n",
      "Body                  999581 non-null object\n",
      "Score                 999581 non-null int64\n",
      "Answer_corpus         999581 non-null object\n",
      "Cleaned_Titles        999581 non-null object\n",
      "Sentence_Embedding    999581 non-null object\n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 91.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999581, 12)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_similar_questions(query):\n",
    "    ''' Function to accept user query and show top 5 similar question alongwith cosine similarity score.\n",
    "        Function accepts one parameter: query (text input)\n",
    "        Processing: Text preprocessing of query, compute sentence embedding and top similar 5 questions.\n",
    "        Returns: Prints dataframe of similar titles and cosine similarity score.\n",
    "    '''\n",
    "    preprocessed_query = text_preprocessing(query)\n",
    "    query_embedding = get_embedding(preprocessed_query)\n",
    "    embeddings = [x for x in df['Sentence_Embedding']]\n",
    "    df['Cosine_sim'] = cosine_similarity(np.array(query_embedding).reshape(1, -1),np.array(embeddings)).T\n",
    "    df.sort_values(by='Cosine_sim', ascending=False, inplace=True)\n",
    "    print(df[['Title','Cosine_sim']].head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ordering in Python (2.4) dictionary' 0.9170001745223999]\n",
      " ['How does Python sort a list of tuples?' 0.9121592044830322]\n",
      " ['Python: sort this dictionary (dict in dict)' 0.9115707874298096]\n",
      " ['Reversible dictionary for python' 0.8975422382354736]\n",
      " ['Python \"extend\" for a dictionary' 0.8960131406784058]]\n",
      "CPU times: user 3.91 s, sys: 1.9 s, total: 5.81 s\n",
      "Wall time: 4.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = 'python sort dictionary'\n",
    "get_similar_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['CSS Performance' 0.9999998807907104]\n",
      " ['CSS Performance' 0.9999998807907104]\n",
      " ['Are CSS selectors a big performance hit?' 0.9220609664916992]\n",
      " ['Performance of tokenizing CSS in PHP' 0.905392050743103]\n",
      " ['IE6 performance with CSS expressions' 0.901648759841919]]\n",
      "CPU times: user 4.49 s, sys: 1.13 s, total: 5.62 s\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = 'CSS Performance'\n",
    "get_similar_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Convert date to datetime in Python' 0.9999998211860657]\n",
      " ['Convert date Python' 0.9570077657699585]\n",
      " ['How do I convert a DateTime to a Date in C#' 0.9416810274124146]\n",
      " ['Convert DateTime to Date' 0.9378916621208191]\n",
      " ['Convert datetime in to date' 0.9378916621208191]]\n",
      "CPU times: user 4.89 s, sys: 1.01 s, total: 5.9 s\n",
      "Wall time: 4.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = 'python convert date to datetime'\n",
    "get_similar_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['How do I create a list of Python lambdas (in a list comprehension/for loop)?'\n",
      "  0.9303664565086365]\n",
      " ['Python: create a list of dictionaries using a list comprehension'\n",
      "  0.9293903112411499]\n",
      " ['How to create a list of lists in PHP?' 0.9288452863693237]\n",
      " ['nesting python list comprehensions to construct a list of lists'\n",
      "  0.9274009466171265]\n",
      " ['Create a dictionary in python which is indexed by lists'\n",
      "  0.9203953742980957]]\n",
      "CPU times: user 4.19 s, sys: 909 ms, total: 5.1 s\n",
      "Wall time: 4.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = 'how to create list of lists in python'\n",
    "get_similar_questions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "get_similar_questions('pd.melt() not working python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satishshah2660/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BAD_UID error while exporting key in CryptoAPI' 0.935831606388092]\n",
      " ['Error correcting key encryption' 0.9212811589241028]\n",
      " [\"Foregin key constraint error but shouldn't be occuring\"\n",
      "  0.9050024747848511]\n",
      " ['trap of primary key error' 0.8889710903167725]\n",
      " ['Error with foreign key' 0.8877214193344116]]\n"
     ]
    }
   ],
   "source": [
    "query = 'key error in w2vec'\n",
    "get_similar_questions_updated(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference:-\n",
    "- Comparing to pre-trained glove model our model seems to be working better.\n",
    "- Its amazing to mark how model learn that list of list in python can be created using list comprehension.\n",
    "- 'w2vec' word isn't present in our vocab still it gave similar results for keyerror occuring in other languages like sql."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
